{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def random_sleep(start, end):\n",
    "    time.sleep(random.randint(start, end))\n",
    "\n",
    "def refresh_wd():\n",
    "    # Define your Chrome options outside this function or ensure they are passed as parameters\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    # Create a new instance of Chrome\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Initialize action chains for possible future actions\n",
    "    actions = ActionChains(wd)\n",
    "    \n",
    "    # Maximize the window to avoid elements being out of view\n",
    "    wd.maximize_window()\n",
    "    \n",
    "    # Navigate to LinkedIn\n",
    "    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "    return wd\n",
    "\n",
    "def login(wd, user_name, password):\n",
    "    try:\n",
    "        # Use WebDriverWait for more reliable element handling\n",
    "        username_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_key\"))\n",
    "        )\n",
    "        username_input.clear()\n",
    "        username_input.send_keys(user_name)\n",
    "        \n",
    "        password_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_password\"))\n",
    "        )\n",
    "        password_input.clear()\n",
    "        password_input.send_keys(password)\n",
    "\n",
    "        print(\"Logging into your LinkedIn account!\")\n",
    "        \n",
    "        # Locate and click the login button\n",
    "        login_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/button'))\n",
    "        )\n",
    "        login_button.click()\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to login: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to login: Could not find one of the elements.\")\n",
    "\n",
    "def security_verification(wd):\n",
    "    otp = input(\"You have been sent a verification code from LinkedIn via your email.\\nPlease input that here: \")\n",
    "    try:\n",
    "        # Use WebDriverWait to wait for the OTP input field to become available\n",
    "        otp_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div/main/form/div[1]/input[15]'))\n",
    "        )\n",
    "        otp_input.clear()\n",
    "        otp_input.send_keys(otp)\n",
    "\n",
    "        # Locate and click the submit button for OTP\n",
    "        submit_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/div/main/form/div[2]/button'))\n",
    "        )\n",
    "        submit_button.click()\n",
    "        print(\"Successfully Authenticated!\")\n",
    "    except TimeoutException:\n",
    "        print(\"Authentication failed: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Authentication failed: Could not find one of the elements.\")\n",
    "\n",
    "def search_query(wd, query):\n",
    "    # Ensure the search input field is visible and clickable\n",
    "    search_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, 'search-global-typeahead__input'))\n",
    "    )\n",
    "    search_field.click()  # Focus on the search field\n",
    "\n",
    "    # Introduce a random delay to mimic human typing speed\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Send the search query\n",
    "    search_field.send_keys(query)\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Press ENTER to execute the search\n",
    "    search_field.send_keys(Keys.ENTER)\n",
    "\n",
    "\n",
    "def add_filters1(wd, location=None, current_company=None, past_company=None):\n",
    "    navigate_to_people_results(wd)\n",
    "    open_all_filters(wd)\n",
    "\n",
    "    if current_company:\n",
    "        apply_filter(wd, current_company, 'company', filter_index=0)\n",
    "    if past_company:\n",
    "        apply_filter(wd, past_company, 'company', filter_index=1)\n",
    "    if location:\n",
    "        apply_filter(wd, location, 'location')\n",
    "\n",
    "    show_all_results(wd)\n",
    "    return wd.current_url\n",
    "\n",
    "def apply_filter(wd, filter_value, filter_type, filter_index=None):\n",
    "    # Locate the \"Add a filter\" button based on the type and index (if applicable)\n",
    "    filter_button_xpath = f\"//*[text()='Add a {filter_type}']\"\n",
    "    filter_buttons = wd.find_elements(By.XPATH, filter_button_xpath)\n",
    "    filter_button = filter_buttons[filter_index] if filter_index is not None else filter_buttons[0]\n",
    "\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", filter_button)\n",
    "    filter_button.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Enter the filter value in the corresponding input field\n",
    "    input_selector = f'input[placeholder=\"Add a {filter_type}\"]'\n",
    "    input_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, input_selector))\n",
    "    )\n",
    "    input_field.click()\n",
    "    input_field.send_keys(filter_value)\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Wait for the listbox to appear and select the correct option\n",
    "    listbox = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'basic-typeahead__triggered-content'))\n",
    "    )\n",
    "    options = listbox.find_elements(By.XPATH, \".//div[@role='option']\")\n",
    "    for option in options:\n",
    "        if filter_value in option.text:\n",
    "            option.click()\n",
    "            break\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def navigate_to_people_results(wd):\n",
    "    try:\n",
    "        # Use WebDriverWait to wait until the button is visible and clickable\n",
    "        people_results_button = WebDriverWait(wd, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'See all people results')]\"))\n",
    "        )\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", people_results_button)\n",
    "        people_page_link = people_results_button.get_attribute(\"href\")\n",
    "        wd.get(people_page_link)\n",
    "        # Wait for a random time between 1 and 2 seconds after loading the page\n",
    "        random_sleep(1, 2)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find the 'See all people results' button within the expected time.\")\n",
    "\n",
    "\n",
    "def open_all_filters(wd):\n",
    "    all_filters = wd.find_element(By.CLASS_NAME, \"relative.mr2\")\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    random_sleep(1,2)\n",
    "\n",
    "\n",
    "def industry_filter(wd, industry_list_num=0):\n",
    "    # Navigate and click on all filters button\n",
    "    navigate_and_click_filters(wd)\n",
    "\n",
    "    # Scroll to and interact with the industry filter\n",
    "    industry_filter_ul = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[text()='Add an industry']/ancestor::ul[1]\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", industry_filter_ul)\n",
    "\n",
    "    # Select the industry based on provided index\n",
    "    select_industry(wd, industry_filter_ul, industry_list_num)\n",
    "\n",
    "    # Click show all results\n",
    "    show_all_results(wd)\n",
    "\n",
    "def navigate_and_click_filters(wd):\n",
    "    all_filters = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, \"relative.mr2\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def select_industry(wd, industry_filter_ul, industry_list_num):\n",
    "    li_elements = industry_filter_ul.find_elements(By.TAG_NAME, \"li\")\n",
    "    if 0 <= industry_list_num < len(li_elements):\n",
    "        li_element = li_elements[industry_list_num]\n",
    "        input_checkbox = li_element.find_element(By.XPATH, \".//input[@type='checkbox']\")\n",
    "        if not input_checkbox.is_selected():\n",
    "            wd.execute_script(\"arguments[0].click();\", input_checkbox)\n",
    "        print(f\"Added industry option {industry_list_num}\")\n",
    "    else:\n",
    "        print(\"Invalid industry index\")\n",
    "\n",
    "def show_all_results(wd):\n",
    "    all_results = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/div[3]/div/div/div[3]/div/button[2]'))\n",
    "    )\n",
    "    all_results.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "\n",
    "def collect_links(wd, page_start, limit, total_results, unlimited=False):\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            handle_retry_search(wd)\n",
    "\n",
    "            if page_start % 10 == 0:\n",
    "                print(f\"Scraping links on Page {page_start}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for person in people_list:\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info(person, names, curr_jobs, summarys, locations, links)\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "\n",
    "            # random_sleep(2, 3)\n",
    "            scroll_down(wd)\n",
    "\n",
    "            if not process_next_page(wd, page_start, limit, unlimited):\n",
    "                break\n",
    "\n",
    "            page_start += 1\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    })\n",
    "\n",
    "def handle_retry_search(wd):\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        print(\"No 'Retry Search' button found, continuing with normal process.\")\n",
    "\n",
    "def collect_person_info(person, names, curr_jobs, summarys, locations, links):\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "def extract_person_details(person, all_links):\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down(wd):\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page(wd, page_start, limit, unlimited):\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    elif not unlimited and page_start == limit + 1:\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def experience_json3(wd, link):\n",
    "    wd.get(link)\n",
    "    about_text = \"\"\n",
    "    WebDriverWait(wd, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))  # Ensure page is loaded\n",
    "\n",
    "    about_text = extract_about_section(wd)\n",
    "    experience_list = extract_experience_section(wd)\n",
    "\n",
    "    return jsonify(about_text, experience_list)\n",
    "\n",
    "def extract_about_section(wd):\n",
    "    try:\n",
    "        about_tags = wd.find_elements(By.XPATH, \"//*[text()='About']\")\n",
    "        if about_tags:\n",
    "            about_tag = about_tags[0]\n",
    "            wd.execute_script(\"arguments[0].scrollIntoView();\", about_tag)\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid red'\", about_tag)\n",
    "            about_section_tag = about_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid blue'\", about_section_tag)\n",
    "            random_sleep(0, 1)\n",
    "            return about_section_tag.text.replace(\"About\\nAbout\\n\", \"\", 1)\n",
    "    except NoSuchElementException:\n",
    "        print(\"About section not found.\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_experience_section(wd):\n",
    "    experience_list = []\n",
    "    try:\n",
    "        experience_tag = wd.find_element(By.XPATH, \"//*[text()='Experience']\")\n",
    "        wd.execute_script(\"arguments[0].scrollIntoView();\", experience_tag)\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", experience_tag)\n",
    "        section_tag = experience_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid blue'\", section_tag)\n",
    "        div_tag = section_tag.find_element(By.XPATH, \".//div[@class='pvs-list__outer-container']\")\n",
    "        jobs = div_tag.find_elements(By.XPATH, \"./ul/li\")\n",
    "\n",
    "        for job in jobs:\n",
    "            process_job_entry(job, experience_list)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Experience section not found.\")\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "def process_job_entry(job, experience_list):\n",
    "    try:\n",
    "        company_name, job_role, job_time = extract_job_details(job)\n",
    "        if company_name:\n",
    "            experience_list.append({'company': company_name, 'job_role': job_role, 'job_time': job_time})\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to process job entry.\")\n",
    "\n",
    "def extract_job_details(job):\n",
    "    company_name = job.find_element(By.CSS_SELECTOR, \"div.display-flex.flex-wrap.align-items-center.full-height span[aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_role = job.find_element(By.XPATH, \".//span[@aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_time = job.find_element(By.CSS_SELECTOR, \"span.t-14.t-normal.t-black--light span.pvs-entity__caption-wrapper\").text.split('·')[0].strip()\n",
    "    return company_name, job_role, job_time\n",
    "\n",
    "\n",
    "def jsonify(about_text, experience_list):\n",
    "    final_dict = {\"About\": about_text, \"Experience\": {}}\n",
    "    for experience in experience_list:\n",
    "        company_dict = {}\n",
    "        if isinstance(experience['job_role'], list) and isinstance(experience['job_time'], list):\n",
    "            for role, time in zip(experience['job_role'], experience['job_time']):\n",
    "                company_dict[role] = time\n",
    "        else:\n",
    "            company_dict[experience['job_role']] = experience['job_time']\n",
    "        final_dict[\"Experience\"][experience['company']] = company_dict\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def get_search_results_number(wd):\n",
    "    try:\n",
    "        results_text = wd.find_element(By.CLASS_NAME, \"pb2.t-black--light.t-14\").text\n",
    "        # Assuming the format of results_text is either \"1,234 results\" or \"Showing 1-10 out of 1,234\"\n",
    "        number = int(results_text.split()[-2].replace(',', ''))\n",
    "        return number\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the search results element.\")\n",
    "        return 0\n",
    "    except ValueError:\n",
    "        print(\"Conversion error, possibly due to unexpected text format.\")\n",
    "        return 0\n",
    "\n",
    "def search_results_more_than_1000(wd):\n",
    "    number = get_search_results_number(wd)\n",
    "    return number > 1000\n",
    "\n",
    "\n",
    "def dataframe_output(wd, search_term, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False, industry=True):\n",
    "    \"\"\"\n",
    "    Conducts a search based on specified parameters and collects results into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance.\n",
    "    search_term (str): The search query.\n",
    "    location (str): Filter by location.\n",
    "    current_company (str): Filter by current company.\n",
    "    past_company (str): Filter by past company.\n",
    "    num_pages (int): Number of pages to scrape.\n",
    "    unlimited (bool): If True, ignores page limits.\n",
    "    industry (bool): If True, applies industry filters if results exceed 1000.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A DataFrame containing the collected data and a list of profile links.\n",
    "    \"\"\"\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    try:\n",
    "        search_query(wd, search_term)  # Assuming search_query now accepts wd as a parameter\n",
    "        \n",
    "        filters_page = add_filters1(wd, location, current_company, past_company)\n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            for i in industry_options:\n",
    "                industry_filter(wd, i)\n",
    "                total_results = get_search_results_number(wd)\n",
    "                sub_df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['Profile Link'])\n",
    "                wd.get(filters_page)\n",
    "        else:\n",
    "            total_results = get_search_results_number(wd)\n",
    "            df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        print(\"Exporting current dataframe...\")\n",
    "\n",
    "    links = list(df['Profile Link'])\n",
    "    return df, links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process(wd, keywords, companies, username, password):\n",
    "    login(wd, username, password)\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link', 'Keyword', 'Company']\n",
    "    job_df = pd.DataFrame(columns=columns)\n",
    "    keyword = \"\"\n",
    "    company = \"\"\n",
    "    query = \"\"\n",
    "\n",
    "    try:\n",
    "        for keyword in keywords:\n",
    "            timeTotal = 0\n",
    "            for i in range(0, len(companies) - 2, 2):\n",
    "\n",
    "                if i != 0:    \n",
    "                    wd.close()\n",
    "                    print(\"Sleeping for 2 minutes\")\n",
    "                    random_sleep(100, 120)\n",
    "                    wd = refresh_wd()\n",
    "\n",
    "                login(wd, username, password)\n",
    "                for company in companies[i:i+2]:\n",
    "                    query = keyword\n",
    "                    location = \"United States\"\n",
    "                    current_company = company\n",
    "                    past_company = None\n",
    "                    unlimited = False\n",
    "                    num_pages = 5\n",
    "                    print(\"Scraping your request...\")\n",
    "                    print((query, current_company))\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    df, links = dataframe_output(wd, query, location, current_company, past_company, num_pages, unlimited, industry=True)\n",
    "                    print(len(df))\n",
    "                    end_time = time.time()\n",
    "                    timeTotal += (end_time - start_time)\n",
    "                    print(end_time - start_time)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        df['Keyword'] = keyword\n",
    "                        df['Company'] = current_company\n",
    "                        df.to_csv(f\"{query.replace(' ', '')}_CompanyDFs/{company.replace(' ', '')}.csv\")\n",
    "                        job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "\n",
    "                    job_df.to_csv(f\"{query.replace(' ', '')}_profiles.csv\")\n",
    "                    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "            print(f\"Average Time per Company Scrape: {(timeTotal / len(companies)) / 60} minutes\")\n",
    "            print(f\"Total Time per Company Scrape: {timeTotal / 60} minutes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if len(df) > 0:\n",
    "            df['Keyword'] = keyword\n",
    "            df['Company'] = company\n",
    "            job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "        job_df.to_csv(f\"{query.replace(' ', '')}_profiles_er.csv\")\n",
    "        print(f'An error occurred: {str(e)} and execution stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "username, password = None, None\n",
    "\n",
    "def set_user_as_PurdueCS(Purdue=True):\n",
    "    if Purdue:\n",
    "        username = \"huoerxiu@gmail.com\"\n",
    "        password = \"PURDUEcs\"\n",
    "    else:\n",
    "        username = \"bhat35@purdue.edu\"\n",
    "        password = \"Aymwos@1977!!\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "wd = webdriver.Chrome(options=options)\n",
    "actions = ActionChains(wd)\n",
    "\n",
    "wd.maximize_window()\n",
    "wd.switch_to.window(wd.current_window_handle)\n",
    "wd.implicitly_wait(10)\n",
    "\n",
    "wd.get('https://www.linkedin.com/')\n",
    "\n",
    "wd.implicitly_wait(1)\n",
    "\n",
    "keywords = ['Machine Learning Engineer']\n",
    "companies = ['Meta', \n",
    "             'Amazon',\n",
    "             'Apple', \n",
    "             'Netflix', \n",
    "             'Google', \n",
    "             'Microsoft', \n",
    "             'Open AI',\n",
    "             'Intel',\n",
    "             'Cicsco',\n",
    "             'Nvidia',\n",
    "             'Salesforce',\n",
    "             'LinkedIn',\n",
    "             'DeepMind',\n",
    "             'IBM',\n",
    "             'Bloomberg',\n",
    "             'Tesla',\n",
    "             'Mayo Clinic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def collect_links(wd, page_start, limit, total_results, unlimited=False, last_person_index=0):\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "    last_processed_person = last_person_index\n",
    "    last_processed_page = page_start\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            handle_retry_search(wd)\n",
    "\n",
    "            if page_start % 10 == 0:\n",
    "                print(f\"Scraping links on Page {page_start}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for index, person in enumerate(people_list):\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info(person, names, curr_jobs, summarys, locations, links)\n",
    "                    last_processed_person = index  # Update last processed person\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "\n",
    "            scroll_down(wd)\n",
    "            if not process_next_page(wd, page_start, limit, unlimited):\n",
    "                break\n",
    "            last_processed_page = page_start\n",
    "            page_start += 1\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "        # Handling to save the state or decide what to do next can be added here\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    }), last_processed_page, last_processed_person\n",
    "\n",
    "def handle_retry_search(wd):\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        print(\"No 'Retry Search' button found, continuing with normal process.\")\n",
    "\n",
    "def collect_person_info(person, names, curr_jobs, summarys, locations, links):\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "def extract_person_details(person, all_links):\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down(wd):\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page(wd, page_start, limit, unlimited):\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    elif not unlimited and page_start == limit + 1:\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_state(state):\n",
    "    with open('state.json', 'w') as f:\n",
    "        json.dump(state, f)\n",
    "\n",
    "def load_state():\n",
    "    try:\n",
    "        with open('state.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def dataframe_output(wd, search_term, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False, industry=True, start_page=1, last_person_index=0):\n",
    "    df = pd.DataFrame(columns=['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link'])\n",
    "    try:\n",
    "        search_query(wd, search_term)\n",
    "        add_filters1(wd, location, current_company, past_company)\n",
    "        \n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            for i in industry_options:\n",
    "                industry_filter(wd, i)\n",
    "                total_results = get_search_results_number(wd)\n",
    "                sub_df, last_page, last_person = collect_links(wd, start_page, num_pages, total_results, unlimited, last_person_index)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['Profile Link'])\n",
    "        else:\n",
    "            df, last_page, last_person = collect_links(wd, start_page, num_pages, 1000, unlimited, last_person_index)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        # Here you might want to save the last state or rethrow the exception to handle it in the main scraping function\n",
    "        raise\n",
    "\n",
    "    return df, last_page, last_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process_rob(wd, keywords, companies, username, password):\n",
    "    state = load_state()\n",
    "    if state is None:\n",
    "        state = {'keyword_index': 0, 'company_index': 0, 'page_start': 1, 'last_person_index': 0}\n",
    "        initial_login = True\n",
    "    else:\n",
    "        initial_login = False\n",
    "\n",
    "    try:\n",
    "        login(wd, username, password) if initial_login else None\n",
    "        for keyword_index in range(state['keyword_index'], len(keywords)):\n",
    "            keyword = keywords[keyword_index]\n",
    "            for company_index in range(state['company_index'], len(companies)):\n",
    "                company = companies[company_index]\n",
    "                try:\n",
    "                    print(f\"Scraping your request for {keyword} at {company}\")\n",
    "                    df, last_page, last_person = dataframe_output(\n",
    "                        wd, keyword, \"United States\", company, None, 5, False, True, \n",
    "                        state['page_start'], state['last_person_index'])\n",
    "                    \n",
    "                    # Reset page start and last person index after successful scrape\n",
    "                    state.update({'page_start': 1, 'last_person_index': 0})\n",
    "                    save_state(state)\n",
    "                except Exception as e:\n",
    "                    print(f\"Encountered an issue with {company}: {e}, saving state and restarting...\")\n",
    "                    state.update({'keyword_index': keyword_index, 'company_index': company_index,\n",
    "                                  'page_start': last_page, 'last_person_index': last_person})\n",
    "                    save_state(state)\n",
    "                    wd.quit()\n",
    "                    wd = refresh_wd()\n",
    "                    login(wd, username, password)\n",
    "                    continue\n",
    "                wd.get('https://www.linkedin.com/')\n",
    "            state.update({'company_index': 0})  # Reset company index after finishing all companies for a keyword\n",
    "        state.update({'keyword_index': 0})  # Reset keyword index after finishing all keywords\n",
    "        save_state(state)\n",
    "    except Exception as e:\n",
    "        print(f'Final error occurred: {str(e)}')\n",
    "\n",
    "    # return pd.read_csv(\"last_saved_dataframe.csv\")  # Assuming your data is saved continuously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into your LinkedIn account!\n",
      "Scraping your request for Machine Learning Engineer at Meta\n",
      "Added industry option 0\n",
      "Something went wrong: collect_links() takes from 4 to 5 positional arguments but 6 were given\n",
      "Encountered an issue with Meta: collect_links() takes from 4 to 5 positional arguments but 6 were given, saving state and restarting...\n",
      "Final error occurred: local variable 'last_page' referenced before assignment\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'last_saved_dataframe.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# main_scraping_process(wd, keywords, companies, username, password)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmain_scraping_process_rob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompanies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m, in \u001b[0;36mmain_scraping_process_rob\u001b[0;34m(wd, keywords, companies, username, password)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast_saved_dataframe.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LinkedInScraping/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LinkedInScraping/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/LinkedInScraping/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LinkedInScraping/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/LinkedInScraping/.venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'last_saved_dataframe.csv'"
     ]
    }
   ],
   "source": [
    "# main_scraping_process(wd, keywords, companies, username, password)\n",
    "set_user_as_PurdueCS(False)\n",
    "main_scraping_process_rob(wd, keywords, companies, username, password)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be8df3a746dc770fc8ec50a8a16d0a923a6575bbbcf271bc91914f68c2488458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
