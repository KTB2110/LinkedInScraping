{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def random_sleep(start, end):\n",
    "    time.sleep(random.randint(start, end))\n",
    "\n",
    "def refresh_wd():\n",
    "    # Define your Chrome options outside this function or ensure they are passed as parameters\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    # Create a new instance of Chrome\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Initialize action chains for possible future actions\n",
    "    actions = ActionChains(wd)\n",
    "    \n",
    "    # Maximize the window to avoid elements being out of view\n",
    "    wd.maximize_window()\n",
    "    \n",
    "    # Navigate to LinkedIn\n",
    "    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "    return wd\n",
    "\n",
    "def login(wd, user_name, password):\n",
    "    try:\n",
    "        # Use WebDriverWait for more reliable element handling\n",
    "        username_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_key\"))\n",
    "        )\n",
    "        username_input.clear()\n",
    "        username_input.send_keys(user_name)\n",
    "        \n",
    "        password_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_password\"))\n",
    "        )\n",
    "        password_input.clear()\n",
    "        password_input.send_keys(password)\n",
    "\n",
    "        print(\"Logging into your LinkedIn account!\")\n",
    "        \n",
    "        # Locate and click the login button\n",
    "        login_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/button'))\n",
    "        )\n",
    "        login_button.click()\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to login: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to login: Could not find one of the elements.\")\n",
    "\n",
    "def security_verification(wd):\n",
    "    otp = input(\"You have been sent a verification code from LinkedIn via your email.\\nPlease input that here: \")\n",
    "    try:\n",
    "        # Use WebDriverWait to wait for the OTP input field to become available\n",
    "        otp_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div/main/form/div[1]/input[15]'))\n",
    "        )\n",
    "        otp_input.clear()\n",
    "        otp_input.send_keys(otp)\n",
    "\n",
    "        # Locate and click the submit button for OTP\n",
    "        submit_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/div/main/form/div[2]/button'))\n",
    "        )\n",
    "        submit_button.click()\n",
    "        print(\"Successfully Authenticated!\")\n",
    "    except TimeoutException:\n",
    "        print(\"Authentication failed: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Authentication failed: Could not find one of the elements.\")\n",
    "\n",
    "def search_query(wd, query):\n",
    "    # Ensure the search input field is visible and clickable\n",
    "    search_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, 'search-global-typeahead__input'))\n",
    "    )\n",
    "    search_field.click()  # Focus on the search field\n",
    "\n",
    "    # Introduce a random delay to mimic human typing speed\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Send the search query\n",
    "    search_field.send_keys(query)\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Press ENTER to execute the search\n",
    "    search_field.send_keys(Keys.ENTER)\n",
    "\n",
    "\n",
    "def add_filters1(wd, location=None, current_company=None, past_company=None):\n",
    "    navigate_to_people_results(wd)\n",
    "    open_all_filters(wd)\n",
    "\n",
    "    if current_company:\n",
    "        apply_filter(wd, current_company, 'company', filter_index=0)\n",
    "    if past_company:\n",
    "        apply_filter(wd, past_company, 'company', filter_index=1)\n",
    "    if location:\n",
    "        apply_filter(wd, location, 'location')\n",
    "\n",
    "    show_all_results(wd)\n",
    "    return wd.current_url\n",
    "\n",
    "def apply_filter(wd, filter_value, filter_type, filter_index=None):\n",
    "    # Locate the \"Add a filter\" button based on the type and index (if applicable)\n",
    "    filter_button_xpath = f\"//*[text()='Add a {filter_type}']\"\n",
    "    filter_buttons = wd.find_elements(By.XPATH, filter_button_xpath)\n",
    "    filter_button = filter_buttons[filter_index] if filter_index is not None else filter_buttons[0]\n",
    "\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", filter_button)\n",
    "    filter_button.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Enter the filter value in the corresponding input field\n",
    "    input_selector = f'input[placeholder=\"Add a {filter_type}\"]'\n",
    "    input_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, input_selector))\n",
    "    )\n",
    "    input_field.click()\n",
    "    input_field.send_keys(filter_value)\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Wait for the listbox to appear and select the correct option\n",
    "    listbox = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'basic-typeahead__triggered-content'))\n",
    "    )\n",
    "    options = listbox.find_elements(By.XPATH, \".//div[@role='option']\")\n",
    "    for option in options:\n",
    "        if filter_value in option.text:\n",
    "            option.click()\n",
    "            break\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def navigate_to_people_results(wd):\n",
    "    try:\n",
    "        # Use WebDriverWait to wait until the button is visible and clickable\n",
    "        people_results_button = WebDriverWait(wd, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'See all people results')]\"))\n",
    "        )\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", people_results_button)\n",
    "        people_page_link = people_results_button.get_attribute(\"href\")\n",
    "        wd.get(people_page_link)\n",
    "        # Wait for a random time between 1 and 2 seconds after loading the page\n",
    "        random_sleep(1, 2)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find the 'See all people results' button within the expected time.\")\n",
    "\n",
    "\n",
    "def open_all_filters(wd):\n",
    "    random_sleep(2,3)\n",
    "    all_filters = wd.find_element(By.CLASS_NAME, \"relative.mr2\")\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    random_sleep(1,2)\n",
    "\n",
    "\n",
    "def industry_filter(wd, industry_list_num=0):\n",
    "    # Navigate and click on all filters button\n",
    "    navigate_and_click_filters(wd)\n",
    "\n",
    "    # Scroll to and interact with the industry filter\n",
    "    industry_filter_ul = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[text()='Add an industry']/ancestor::ul[1]\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", industry_filter_ul)\n",
    "\n",
    "    # Select the industry based on provided index\n",
    "    select_industry(wd, industry_filter_ul, industry_list_num)\n",
    "\n",
    "    # Click show all results\n",
    "    show_all_results(wd)\n",
    "\n",
    "def navigate_and_click_filters(wd):\n",
    "    all_filters = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, \"relative.mr2\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def select_industry(wd, industry_filter_ul, industry_list_num):\n",
    "    li_elements = industry_filter_ul.find_elements(By.TAG_NAME, \"li\")\n",
    "    if 0 <= industry_list_num < len(li_elements):\n",
    "        li_element = li_elements[industry_list_num]\n",
    "        input_checkbox = li_element.find_element(By.XPATH, \".//input[@type='checkbox']\")\n",
    "        if not input_checkbox.is_selected():\n",
    "            wd.execute_script(\"arguments[0].click();\", input_checkbox)\n",
    "        print(f\"Added industry option {industry_list_num}\")\n",
    "    else:\n",
    "        print(\"Invalid industry index\")\n",
    "\n",
    "def show_all_results(wd):\n",
    "    all_results = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/div[3]/div/div/div[3]/div/button[2]'))\n",
    "    )\n",
    "    all_results.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "\n",
    "def collect_links(wd, page_start, limit, total_results, unlimited=False):\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            handle_retry_search(wd)\n",
    "\n",
    "            if page_start % 10 == 0:\n",
    "                print(f\"Scraping links on Page {page_start}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for person in people_list:\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info(person, names, curr_jobs, summarys, locations, links)\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "\n",
    "            # random_sleep(2, 3)\n",
    "            scroll_down(wd)\n",
    "\n",
    "            if not process_next_page(wd, page_start, limit, unlimited):\n",
    "                break\n",
    "\n",
    "            page_start += 1\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    })\n",
    "\n",
    "def handle_retry_search(wd):\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        print(\"No 'Retry Search' button found, continuing with normal process.\")\n",
    "\n",
    "def collect_person_info(person, names, curr_jobs, summarys, locations, links):\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "def extract_person_details(person, all_links):\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down(wd):\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page(wd, page_start, limit, unlimited):\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    elif not unlimited and page_start == limit + 1:\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def experience_json3(wd, link):\n",
    "    wd.get(link)\n",
    "    about_text = \"\"\n",
    "    WebDriverWait(wd, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))  # Ensure page is loaded\n",
    "\n",
    "    about_text = extract_about_section(wd)\n",
    "    experience_list = extract_experience_section(wd)\n",
    "\n",
    "    return jsonify(about_text, experience_list)\n",
    "\n",
    "def extract_about_section(wd):\n",
    "    try:\n",
    "        about_tags = wd.find_elements(By.XPATH, \"//*[text()='About']\")\n",
    "        if about_tags:\n",
    "            about_tag = about_tags[0]\n",
    "            wd.execute_script(\"arguments[0].scrollIntoView();\", about_tag)\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid red'\", about_tag)\n",
    "            about_section_tag = about_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid blue'\", about_section_tag)\n",
    "            random_sleep(0, 1)\n",
    "            return about_section_tag.text.replace(\"About\\nAbout\\n\", \"\", 1)\n",
    "    except NoSuchElementException:\n",
    "        print(\"About section not found.\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_experience_section(wd):\n",
    "    experience_list = []\n",
    "    try:\n",
    "        experience_tag = wd.find_element(By.XPATH, \"//*[text()='Experience']\")\n",
    "        wd.execute_script(\"arguments[0].scrollIntoView();\", experience_tag)\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", experience_tag)\n",
    "        section_tag = experience_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid blue'\", section_tag)\n",
    "        div_tag = section_tag.find_element(By.XPATH, \".//div[@class='pvs-list__outer-container']\")\n",
    "        jobs = div_tag.find_elements(By.XPATH, \"./ul/li\")\n",
    "\n",
    "        for job in jobs:\n",
    "            process_job_entry(job, experience_list)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Experience section not found.\")\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "def process_job_entry(job, experience_list):\n",
    "    try:\n",
    "        company_name, job_role, job_time = extract_job_details(job)\n",
    "        if company_name:\n",
    "            experience_list.append({'company': company_name, 'job_role': job_role, 'job_time': job_time})\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to process job entry.\")\n",
    "\n",
    "def extract_job_details(job):\n",
    "    company_name = job.find_element(By.CSS_SELECTOR, \"div.display-flex.flex-wrap.align-items-center.full-height span[aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_role = job.find_element(By.XPATH, \".//span[@aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_time = job.find_element(By.CSS_SELECTOR, \"span.t-14.t-normal.t-black--light span.pvs-entity__caption-wrapper\").text.split('·')[0].strip()\n",
    "    return company_name, job_role, job_time\n",
    "\n",
    "\n",
    "def jsonify(about_text, experience_list):\n",
    "    final_dict = {\"About\": about_text, \"Experience\": {}}\n",
    "    for experience in experience_list:\n",
    "        company_dict = {}\n",
    "        if isinstance(experience['job_role'], list) and isinstance(experience['job_time'], list):\n",
    "            for role, time in zip(experience['job_role'], experience['job_time']):\n",
    "                company_dict[role] = time\n",
    "        else:\n",
    "            company_dict[experience['job_role']] = experience['job_time']\n",
    "        final_dict[\"Experience\"][experience['company']] = company_dict\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def get_search_results_number(wd):\n",
    "    try:\n",
    "        results_text = wd.find_element(By.CLASS_NAME, \"pb2.t-black--light.t-14\").text\n",
    "        # Assuming the format of results_text is either \"1,234 results\" or \"Showing 1-10 out of 1,234\"\n",
    "        number = int(results_text.split()[-2].replace(',', ''))\n",
    "        return number\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the search results element.\")\n",
    "        return 0\n",
    "    except ValueError:\n",
    "        print(\"Conversion error, possibly due to unexpected text format.\")\n",
    "        return 0\n",
    "\n",
    "def search_results_more_than_1000(wd):\n",
    "    number = get_search_results_number(wd)\n",
    "    return number > 1000\n",
    "\n",
    "\n",
    "def dataframe_output(wd, search_term, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False, industry=True):\n",
    "    \"\"\"\n",
    "    Conducts a search based on specified parameters and collects results into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance.\n",
    "    search_term (str): The search query.\n",
    "    location (str): Filter by location.\n",
    "    current_company (str): Filter by current company.\n",
    "    past_company (str): Filter by past company.\n",
    "    num_pages (int): Number of pages to scrape.\n",
    "    unlimited (bool): If True, ignores page limits.\n",
    "    industry (bool): If True, applies industry filters if results exceed 1000.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A DataFrame containing the collected data and a list of profile links.\n",
    "    \"\"\"\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    try:\n",
    "        search_query(wd, search_term)  # Assuming search_query now accepts wd as a parameter\n",
    "        \n",
    "        filters_page = add_filters1(wd, location, current_company, past_company)\n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            for i in industry_options:\n",
    "                industry_filter(wd, i)\n",
    "                total_results = get_search_results_number(wd)\n",
    "                sub_df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['Profile Link'])\n",
    "                wd.get(filters_page)\n",
    "        else:\n",
    "            total_results = get_search_results_number(wd)\n",
    "            df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        print(\"Exporting current dataframe...\")\n",
    "\n",
    "    links = list(df['Profile Link'])\n",
    "    return df, links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process(wd, keywords, companies, username, password):\n",
    "    login(wd, username, password)\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link', 'Keyword', 'Company']\n",
    "    job_df = pd.DataFrame(columns=columns)\n",
    "    keyword = \"\"\n",
    "    company = \"\"\n",
    "    query = \"\"\n",
    "\n",
    "    try:\n",
    "        for keyword in keywords:\n",
    "            timeTotal = 0\n",
    "            for i in range(0, len(companies) - 2, 2):\n",
    "\n",
    "                if i != 0:    \n",
    "                    wd.close()\n",
    "                    print(\"Sleeping for 2 minutes\")\n",
    "                    random_sleep(100, 120)\n",
    "                    wd = refresh_wd()\n",
    "\n",
    "                login(wd, username, password)\n",
    "                for company in companies[i:i+2]:\n",
    "                    query = keyword\n",
    "                    location = \"United States\"\n",
    "                    current_company = company\n",
    "                    past_company = None\n",
    "                    unlimited = False\n",
    "                    num_pages = 5\n",
    "                    print(\"Scraping your request...\")\n",
    "                    print((query, current_company))\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    df, links = dataframe_output(wd, query, location, current_company, past_company, num_pages, unlimited, industry=True)\n",
    "                    print(len(df))\n",
    "                    end_time = time.time()\n",
    "                    timeTotal += (end_time - start_time)\n",
    "                    print(end_time - start_time)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        df['Keyword'] = keyword\n",
    "                        df['Company'] = current_company\n",
    "                        df.to_csv(f\"{query.replace(' ', '')}_CompanyDFs/{company.replace(' ', '')}.csv\")\n",
    "                        job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "\n",
    "                    job_df.to_csv(f\"{query.replace(' ', '')}_profiles.csv\")\n",
    "                    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "            print(f\"Average Time per Company Scrape: {(timeTotal / len(companies)) / 60} minutes\")\n",
    "            print(f\"Total Time per Company Scrape: {timeTotal / 60} minutes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if len(df) > 0:\n",
    "            df['Keyword'] = keyword\n",
    "            df['Company'] = company\n",
    "            job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "        job_df.to_csv(f\"{query.replace(' ', '')}_profiles_er.csv\")\n",
    "        print(f'An error occurred: {str(e)} and execution stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "username, password = None, None\n",
    "\n",
    "def set_user_as_PurdueCS(Purdue=True):\n",
    "    global username, password\n",
    "    if Purdue:\n",
    "        username = \"huoerxiu@gmail.com\"\n",
    "        password = \"PURDUEcs\"\n",
    "    else:\n",
    "        username = \"bhat35@purdue.edu\"\n",
    "        password = \"Aymwos@1977!!\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "wd = webdriver.Chrome(options=options)\n",
    "\n",
    "# wd = webdriver.Chrome(ChromeDriverManager().install())\n",
    "actions = ActionChains(wd)\n",
    "\n",
    "wd.maximize_window()\n",
    "wd.switch_to.window(wd.current_window_handle)\n",
    "wd.implicitly_wait(10)\n",
    "\n",
    "wd.get('https://www.linkedin.com/')\n",
    "\n",
    "wd.implicitly_wait(1)\n",
    "\n",
    "keywords = ['Machine Learning Engineer']\n",
    "companies = ['Meta', \n",
    "             'Amazon',\n",
    "             'Apple', \n",
    "             'Netflix', \n",
    "             'Google', \n",
    "             'Microsoft', \n",
    "             'Open AI',\n",
    "             'Intel',\n",
    "             'Cicsco',\n",
    "             'Nvidia',\n",
    "             'Salesforce',\n",
    "             'LinkedIn',\n",
    "             'DeepMind',\n",
    "             'IBM',\n",
    "             'Bloomberg',\n",
    "             'Tesla',\n",
    "             'Mayo Clinic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def collect_links1(wd, search_term, current_company, state, limit, total_results, unlimited=False, output_file='continuous_data_scrape.csv'):\n",
    "    \n",
    "    state = load_state()\n",
    "    page_to_start_at = state['page_start']\n",
    "\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "    print(\"Unlimited:\", unlimited)\n",
    "\n",
    "    ### IMPLEMENT THE BELOW METHOD\n",
    "    # go_to_page(page_to_start_at)\n",
    "\n",
    "    curr_page = page_to_start_at\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            p_names, p_curr_jobs, p_summarys, p_locations, p_links = [], [], [], [], []\n",
    "            handle_retry_search1(wd)\n",
    "\n",
    "            if curr_page % 10 == 0:\n",
    "                print(f\"Scraping links on Page {curr_page}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for index, person in enumerate(people_list):\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info1(person, names, curr_jobs, summarys, locations, links) # Adding to general lists\n",
    "                    collect_person_info1(person, p_names, p_curr_jobs, p_summarys, p_locations, p_links) # Adding to lists of specific page\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'Name': p_names,\n",
    "                'Current Job': p_curr_jobs,\n",
    "                'Relevant Experience to Keyword': p_summarys,\n",
    "                'Location': p_locations,\n",
    "                'Profile Link': p_links\n",
    "            })\n",
    "\n",
    "            df['Keyword'] = search_term\n",
    "            df['Company'] = current_company\n",
    "\n",
    "            if not os.path.isfile(output_file):\n",
    "                df.to_csv(output_file, index=False)  # Create file and write data\n",
    "            else:\n",
    "                df.to_csv(output_file, mode='a', header=False, index=False)  # Append data to existing file\n",
    "\n",
    "            scroll_down1(wd)\n",
    "            if not process_next_page1(wd, curr_page, limit, unlimited):\n",
    "                break\n",
    "\n",
    "            curr_page += 1\n",
    "            state.update({'page_start': curr_page})\n",
    "            save_state(state)\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "        # Handling to save the state or decide what to do next can be added here\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    })\n",
    "\n",
    "def handle_retry_search1(wd):\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 1).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        not_found = True\n",
    "\n",
    "def collect_person_info1(person, names, curr_jobs, summarys, locations, links):\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details1(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "    \n",
    "\n",
    "def extract_person_details1(person, all_links):\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down1(wd):\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page1(wd, curr_page, limit, unlimited):\n",
    "    print(\"Unlimited: (process_next_page)\", unlimited)\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if not unlimited and curr_page == limit:\n",
    "        return False\n",
    "\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_state(state):\n",
    "    with open('state.json', 'w') as f:\n",
    "        json.dump(state, f)\n",
    "\n",
    "def load_state():\n",
    "    try:\n",
    "        with open('state.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def dataframe_output1(wd, search_term, state, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False, industry=True):\n",
    "    df = pd.DataFrame(columns=['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link'])\n",
    "    try:\n",
    "        search_query(wd, search_term)\n",
    "        # add_filters1(wd, location, current_company, past_company)\n",
    "        \n",
    "        filters_page = add_filters1(wd, location, current_company, past_company)\n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            for i in industry_options:\n",
    "                industry_filter(wd, i)\n",
    "                total_results = get_search_results_number(wd)\n",
    "                sub_df = collect_links1(wd, search_term, current_company, state, num_pages, total_results, unlimited)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['Profile Link'])\n",
    "                wd.get(filters_page)\n",
    "                random_sleep(1,2)\n",
    "        else:\n",
    "            df = collect_links1(wd, search_term, current_company, state, num_pages, 1000, unlimited)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        # Here you might want to save the last state or rethrow the exception to handle it in the main scraping function\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process_rob(wd, keywords, companies, username, password, output_file='scraped_data.csv'):\n",
    "    state = load_state()\n",
    "    if state is None:\n",
    "        state = {'keyword_index': 0, 'company_index': 0, 'page_start': 1}\n",
    "        save_state(state)\n",
    "        initial_login = True\n",
    "    else:\n",
    "        initial_login = False\n",
    "\n",
    "    try:\n",
    "        # login(wd, username, password) if initial_login else None\n",
    "        keyword_start_index = state['keyword_index']\n",
    "        company_start_index = state['company_index']\n",
    "        for keyword_index in range(keyword_start_index, len(keywords)):\n",
    "            keyword = keywords[keyword_index]\n",
    "            for company_index in range(company_start_index, len(companies)):\n",
    "                company = companies[company_index]\n",
    "                try:\n",
    "                    print(f\"Scraping your request for {keyword} at {company}\")\n",
    "                    df = dataframe_output1(\n",
    "                        wd=wd, search_term=keyword, state=state, location=\"United States\", current_company=company, past_company=None, num_pages=2, unlimited=False, industry=False)\n",
    "                    \n",
    "                    df['Keyword'] = keyword\n",
    "                    df['Company'] = company\n",
    "                    if not os.path.isfile(output_file):\n",
    "                        df.to_csv(output_file, index=False)  # Create file and write data\n",
    "                    else:\n",
    "                        df.to_csv(output_file, mode='a', header=False, index=False)  # Append data to existing file\n",
    "\n",
    "                    # Reset page start and last person index after successful scrape\n",
    "                    state.update({'company_index': company_index+1, 'page_start': 1})\n",
    "                    save_state(state)\n",
    "                except Exception as e:\n",
    "                    print(f\"Encountered an issue with {company}: {e}, saving state and restarting...\")\n",
    "                    state = load_state()\n",
    "                    print(state)\n",
    "                    # state.update({'keyword_index': keyword_index, 'company_index': company_index,\n",
    "                    #               'page_start': last_page, 'last_person_index': last_person})\n",
    "                    # save_state(state)\n",
    "                    # wd.quit()\n",
    "                    # wd = refresh_wd()\n",
    "                    login(wd, username, password)\n",
    "                    continue\n",
    "                wd.get('https://www.linkedin.com/')\n",
    "            state.update({'company_index': 0})  # Reset company index after finishing all companies for a keyword\n",
    "        state.update({'keyword_index': 0})  # Reset keyword index after finishing all keywords\n",
    "        save_state(state)\n",
    "    except Exception as e:\n",
    "        print(f'Final error occurred: {str(e)}')\n",
    "\n",
    "    # return pd.read_csv(\"last_saved_dataframe.csv\")  # Assuming your data is saved continuously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bhat35@purdue.edu Aymwos@1977!!\n"
     ]
    }
   ],
   "source": [
    "set_user_as_PurdueCS(Purdue=False)\n",
    "print(username,password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping your request for Machine Learning Engineer at Meta\n",
      "Unlimited: False\n",
      "Required Element Not Found...Moving on\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".entity-result__title-text.t-16 a span[aria-hidden='true']\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000104b9a510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000104b92e58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001047c419c chromedriver + 278940\n",
      "3   chromedriver                        0x00000001048062c4 chromedriver + 549572\n",
      "4   chromedriver                        0x00000001047fc85c chromedriver + 510044\n",
      "5   chromedriver                        0x000000010483ec5c chromedriver + 781404\n",
      "6   chromedriver                        0x00000001047fb004 chromedriver + 503812\n",
      "7   chromedriver                        0x00000001047fb9ec chromedriver + 506348\n",
      "8   chromedriver                        0x0000000104b62558 chromedriver + 4072792\n",
      "9   chromedriver                        0x0000000104b67004 chromedriver + 4091908\n",
      "10  chromedriver                        0x0000000104b4979c chromedriver + 3970972\n",
      "11  chromedriver                        0x0000000104b678ec chromedriver + 4094188\n",
      "12  chromedriver                        0x0000000104b3c71c chromedriver + 3917596\n",
      "13  chromedriver                        0x0000000104b84b50 chromedriver + 4213584\n",
      "14  chromedriver                        0x0000000104b84ccc chromedriver + 4213964\n",
      "15  chromedriver                        0x0000000104b92a50 chromedriver + 4270672\n",
      "16  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "Unlimited: (process_next_page) False\n",
      "Unlimited: (process_next_page) False\n",
      "Scraping your request for Machine Learning Engineer at Amazon\n",
      "Unlimited: False\n",
      "Required Element Not Found...Moving on\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".entity-result__title-text.t-16 a span[aria-hidden='true']\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000104b9a510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000104b92e58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001047c419c chromedriver + 278940\n",
      "3   chromedriver                        0x00000001048062c4 chromedriver + 549572\n",
      "4   chromedriver                        0x00000001047fc85c chromedriver + 510044\n",
      "5   chromedriver                        0x000000010483ec5c chromedriver + 781404\n",
      "6   chromedriver                        0x00000001047fb004 chromedriver + 503812\n",
      "7   chromedriver                        0x00000001047fb9ec chromedriver + 506348\n",
      "8   chromedriver                        0x0000000104b62558 chromedriver + 4072792\n",
      "9   chromedriver                        0x0000000104b67004 chromedriver + 4091908\n",
      "10  chromedriver                        0x0000000104b4979c chromedriver + 3970972\n",
      "11  chromedriver                        0x0000000104b678ec chromedriver + 4094188\n",
      "12  chromedriver                        0x0000000104b3c71c chromedriver + 3917596\n",
      "13  chromedriver                        0x0000000104b84b50 chromedriver + 4213584\n",
      "14  chromedriver                        0x0000000104b84ccc chromedriver + 4213964\n",
      "15  chromedriver                        0x0000000104b92a50 chromedriver + 4270672\n",
      "16  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "Unlimited: (process_next_page) False\n",
      "Unlimited: (process_next_page) False\n",
      "Scraping your request for Machine Learning Engineer at Apple\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# main_scraping_process(wd, keywords, companies, username, password)846448\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m main_scraping_process_rob(wd, keywords, companies, username, password)\n",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 9\u001b[0m in \u001b[0;36mmain_scraping_process_rob\u001b[0;34m(wd, keywords, companies, username, password, output_file)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScraping your request for \u001b[39m\u001b[39m{\u001b[39;00mkeyword\u001b[39m}\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m{\u001b[39;00mcompany\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     df \u001b[39m=\u001b[39m dataframe_output1(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         wd\u001b[39m=\u001b[39;49mwd, search_term\u001b[39m=\u001b[39;49mkeyword, state\u001b[39m=\u001b[39;49mstate, location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mUnited States\u001b[39;49m\u001b[39m\"\u001b[39;49m, current_company\u001b[39m=\u001b[39;49mcompany, past_company\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, num_pages\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, unlimited\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, industry\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mKeyword\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m keyword\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mCompany\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m company\n",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 9\u001b[0m in \u001b[0;36mdataframe_output1\u001b[0;34m(wd, search_term, state, location, current_company, past_company, num_pages, unlimited, industry)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m search_query(wd, search_term)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39m# add_filters1(wd, location, current_company, past_company)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m filters_page \u001b[39m=\u001b[39m add_filters1(wd, location, current_company, past_company)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39mif\u001b[39;00m industry \u001b[39mand\u001b[39;00m search_results_more_than_1000(wd):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     industry_options \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]\n",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 9\u001b[0m in \u001b[0;36madd_filters1\u001b[0;34m(wd, location, current_company, past_company)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_filters1\u001b[39m(wd, location\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, current_company\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, past_company\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     navigate_to_people_results(wd)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     open_all_filters(wd)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39mif\u001b[39;00m current_company:\n",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 9\u001b[0m in \u001b[0;36mnavigate_to_people_results\u001b[0;34m(wd)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     wd\u001b[39m.\u001b[39mget(people_page_link)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     \u001b[39m# Wait for a random time between 1 and 2 seconds after loading the page\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m     random_sleep(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mexcept\u001b[39;00m TimeoutException:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFailed to find the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSee all people results\u001b[39m\u001b[39m'\u001b[39m\u001b[39m button within the expected time.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 9\u001b[0m in \u001b[0;36mrandom_sleep\u001b[0;34m(start, end)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_sleep\u001b[39m(start, end):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(random\u001b[39m.\u001b[39;49mrandint(start, end))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main_scraping_process(wd, keywords, companies, username, password)846448\n",
    "main_scraping_process_rob(wd, keywords, companies, username, password)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be8df3a746dc770fc8ec50a8a16d0a923a6575bbbcf271bc91914f68c2488458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
