{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def random_sleep(start, end):\n",
    "    \"\"\"\n",
    "    Introduces a random sleep interval between the specified start and end range to simulate human-like delays in script execution.\n",
    "\n",
    "    Args:\n",
    "        start (int): The minimum number of seconds to sleep.\n",
    "        end (int): The maximum number of seconds to sleep.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    time.sleep(random.randint(start, end))\n",
    "\n",
    "def refresh_wd():\n",
    "    \"\"\"\n",
    "    Initializes and returns a new instance of the Chrome WebDriver with specified options to avoid sandboxing and disable shared memory usage. It maximizes the browser window, navigates to LinkedIn's homepage, and sets up action chains for future interactions.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        WebDriver: A new instance of the Chrome WebDriver.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define your Chrome options outside this function or ensure they are passed as parameters\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    # Create a new instance of Chrome\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Initialize action chains for possible future actions\n",
    "    actions = ActionChains(wd)\n",
    "    \n",
    "    # Maximize the window to avoid elements being out of view\n",
    "    wd.maximize_window()\n",
    "    \n",
    "    # Navigate to LinkedIn\n",
    "    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "    return wd\n",
    "\n",
    "def login(wd, user_name, password):\n",
    "    \"\"\"\n",
    "    Logs into a LinkedIn account using the provided WebDriver instance, username, and password. It waits for the relevant input fields to become available, enters the credentials, and clicks the login button.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        user_name (str): The LinkedIn account username.\n",
    "        password (str): The LinkedIn account password.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Use WebDriverWait for more reliable element handling\n",
    "        username_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_key\"))\n",
    "        )\n",
    "        username_input.clear()\n",
    "        username_input.send_keys(user_name)\n",
    "        \n",
    "        password_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_password\"))\n",
    "        )\n",
    "        password_input.clear()\n",
    "        password_input.send_keys(password)\n",
    "\n",
    "        print(\"Logging into your LinkedIn account!\")\n",
    "        \n",
    "        # Locate and click the login button\n",
    "        login_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/button'))\n",
    "        )\n",
    "        login_button.click()\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to login: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to login: Could not find one of the elements.\")\n",
    "\n",
    "def login_alternative(wd, user_name, password):\n",
    "    \"\"\"\n",
    "    Handles logging into LinkedIn using the alternative method. This method clicks the \"Sign in\" button, enters the username and password into the appropriate fields, and then clicks the \"Sign in\" button again.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        user_name (str): The LinkedIn account username.\n",
    "        password (str): The LinkedIn account password.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sign_in_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//a[@class=\"nav__button-secondary btn-md btn-secondary-emphasis\"]'))\n",
    "        )\n",
    "        sign_in_button.click()\n",
    "\n",
    "        username_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"username\"))\n",
    "        )\n",
    "        username_input.clear()\n",
    "        username_input.send_keys(user_name)\n",
    "\n",
    "        password_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"password\"))\n",
    "        )\n",
    "        password_input.clear()\n",
    "        password_input.send_keys(password)\n",
    "\n",
    "        print(\"Logging into your LinkedIn account using alternative method!\")\n",
    "        \n",
    "        login_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//button[@class=\"btn__primary--large from__button--floating\"]'))\n",
    "        )\n",
    "        login_button.click()\n",
    "    except (TimeoutException, NoSuchElementException) as e:\n",
    "        print(f\"Failed to login using alternative method: {str(e)}\")\n",
    "\n",
    "def login_with_fallback(wd, user_name, password):\n",
    "    \"\"\"\n",
    "    Attempts to log into LinkedIn using the primary login method. If an exception occurs, it falls back to the alternative login method.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        user_name (str): The LinkedIn account username.\n",
    "        password (str): The LinkedIn account password.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        login(wd, user_name, password)\n",
    "    except Exception as e:\n",
    "        print(f\"Primary login method failed: {str(e)}\")\n",
    "        login_alternative(wd, user_name, password)\n",
    "\n",
    "def security_verification(wd):\n",
    "    \"\"\"\n",
    "    Handles the security verification process by prompting the user to input a verification code received from LinkedIn. It waits for the OTP input field to become available, enters the OTP, and submits it.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    otp = input(\"You have been sent a verification code from LinkedIn via your email.\\nPlease input that here: \")\n",
    "    try:\n",
    "        # Use WebDriverWait to wait for the OTP input field to become available\n",
    "        otp_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div/main/form/div[1]/input[15]'))\n",
    "        )\n",
    "        otp_input.clear()\n",
    "        otp_input.send_keys(otp)\n",
    "\n",
    "        # Locate and click the submit button for OTP\n",
    "        submit_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/div/main/form/div[2]/button'))\n",
    "        )\n",
    "        submit_button.click()\n",
    "        print(\"Successfully Authenticated!\")\n",
    "    except TimeoutException:\n",
    "        print(\"Authentication failed: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Authentication failed: Could not find one of the elements.\")\n",
    "\n",
    "def search_query(wd, query):\n",
    "    \"\"\"\n",
    "    Performs a search on LinkedIn using the provided search query. It waits for the search input field to become clickable, enters the search query, and executes the search.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        query (str): The search query to be entered.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the search input field is visible and clickable\n",
    "    search_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, 'search-global-typeahead__input'))\n",
    "    )\n",
    "    search_field.click()  # Focus on the search field\n",
    "\n",
    "    # Introduce a random delay to mimic human typing speed\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Send the search query\n",
    "    search_field.send_keys(query)\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Press ENTER to execute the search\n",
    "    search_field.send_keys(Keys.ENTER)\n",
    "\n",
    "\n",
    "def add_filters1(wd, location=None, current_company=None, past_company=None):\n",
    "    \"\"\"\n",
    "    Applies search filters on LinkedIn based on location, current company, and past company. It navigates to the people results, opens the all filters menu, and applies the specified filters.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        location (str, optional): The location filter to be applied.\n",
    "        current_company (str, optional): The current company filter to be applied.\n",
    "        past_company (str, optional): The past company filter to be applied.\n",
    "\n",
    "    Returns:\n",
    "        str: The current URL after applying the filters.\n",
    "    \"\"\"\n",
    "\n",
    "    navigate_to_people_results(wd)\n",
    "    open_all_filters(wd)\n",
    "\n",
    "    if current_company:\n",
    "        apply_filter(wd, current_company, 'company', filter_index=0)\n",
    "    if past_company:\n",
    "        apply_filter(wd, past_company, 'company', filter_index=1)\n",
    "    if location:\n",
    "        apply_filter(wd, location, 'location')\n",
    "\n",
    "    show_all_results(wd)\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    url = wd.current_url\n",
    "    return url\n",
    "\n",
    "def apply_filter(wd, filter_value, filter_type, filter_index=None):\n",
    "    \"\"\"\n",
    "    Applies a specified filter on LinkedIn based on the filter type and index. It waits for the filter button to become clickable, enters the filter value, and selects the correct option from the listbox.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        filter_value (str): The value to be applied in the filter.\n",
    "        filter_type (str): The type of filter to be applied (e.g., 'company', 'location').\n",
    "        filter_index (int, optional): The index of the filter button if multiple buttons exist.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Locate the \"Add a filter\" button based on the type and index (if applicable)\n",
    "    filter_button_xpath = f\"//*[text()='Add a {filter_type}']\"\n",
    "    filter_buttons = wd.find_elements(By.XPATH, filter_button_xpath)\n",
    "    filter_button = filter_buttons[filter_index] if filter_index is not None else filter_buttons[0]\n",
    "\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", filter_button)\n",
    "    filter_button.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Enter the filter value in the corresponding input field\n",
    "    input_selector = f'input[placeholder=\"Add a {filter_type}\"]'\n",
    "    input_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, input_selector))\n",
    "    )\n",
    "    input_field.click()\n",
    "    input_field.send_keys(filter_value)\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Wait for the listbox to appear and select the correct option\n",
    "    listbox = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'basic-typeahead__triggered-content'))\n",
    "    )\n",
    "    options = listbox.find_elements(By.XPATH, \".//div[@role='option']\")\n",
    "    for option in options:\n",
    "        if filter_value in option.text:\n",
    "            option.click()\n",
    "            break\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def navigate_to_people_results(wd):\n",
    "    \"\"\"\n",
    "    Navigates to the LinkedIn people results page by clicking on the \"See all people results\" button. It waits for the button to become clickable and then clicks it.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Use WebDriverWait to wait until the button is visible and clickable\n",
    "        people_results_button = WebDriverWait(wd, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'See all people results')]\"))\n",
    "        )\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", people_results_button)\n",
    "        people_page_link = people_results_button.get_attribute(\"href\")\n",
    "        wd.get(people_page_link)\n",
    "        # Wait for a random time between 1 and 2 seconds after loading the page\n",
    "        random_sleep(1, 2)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find the 'See all people results' button within the expected time.\")\n",
    "\n",
    "\n",
    "def open_all_filters(wd):\n",
    "    \"\"\"\n",
    "    Opens the \"All filters\" menu on LinkedIn's search results page. It waits for the \"All filters\" button to become clickable and then clicks it.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    random_sleep(2,3)\n",
    "    all_filters = wd.find_element(By.CLASS_NAME, \"relative.mr2\")\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    random_sleep(1,2)\n",
    "\n",
    "\n",
    "def industry_filter(wd, industry_list_num=0):\n",
    "    \"\"\"\n",
    "    Applies an industry filter based on the provided index. It navigates to the all filters menu, scrolls to the industry filter, and selects the industry option based on the given index.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        industry_list_num (int): The index of the industry option to be selected.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Navigate and click on all filters button\n",
    "    navigate_and_click_filters(wd)\n",
    "\n",
    "    # Scroll to and interact with the industry filter\n",
    "    industry_filter_ul = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[text()='Add an industry']/ancestor::ul[1]\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", industry_filter_ul)\n",
    "\n",
    "    # Select the industry based on provided index\n",
    "    select_industry(wd, industry_filter_ul, industry_list_num)\n",
    "\n",
    "    # Click show all results\n",
    "    show_all_results(wd)\n",
    "\n",
    "def navigate_and_click_filters(wd):\n",
    "    \"\"\"\n",
    "    Navigates to and clicks the \"All filters\" button on LinkedIn's search results page. It waits for the button to become clickable and then clicks it.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    all_filters = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, \"relative.mr2\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def select_industry(wd, industry_filter_ul, industry_list_num):\n",
    "    \"\"\"\n",
    "    Selects an industry filter option based on the provided index. It locates the industry filter element, scrolls to it, and selects the specified option.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        industry_filter_ul (WebElement): The unordered list element containing industry filter options.\n",
    "        industry_list_num (int): The index of the industry option to be selected.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    li_elements = industry_filter_ul.find_elements(By.TAG_NAME, \"li\")\n",
    "    if 0 <= industry_list_num < len(li_elements):\n",
    "        li_element = li_elements[industry_list_num]\n",
    "        input_checkbox = li_element.find_element(By.XPATH, \".//input[@type='checkbox']\")\n",
    "        if not input_checkbox.is_selected():\n",
    "            wd.execute_script(\"arguments[0].click();\", input_checkbox)\n",
    "        print(f\"Added industry option {industry_list_num}\")\n",
    "    else:\n",
    "        print(\"Invalid industry index\")\n",
    "\n",
    "def show_all_results(wd):\n",
    "    \"\"\"\n",
    "    Clicks the \"Show all results\" button on LinkedIn's search filters page. It waits for the button to become clickable and then clicks it.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    all_results = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/div[3]/div/div/div[3]/div/button[2]'))\n",
    "    )\n",
    "    all_results.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "\n",
    "def collect_links(wd, page_start, limit, total_results, unlimited=False):\n",
    "    \"\"\"\n",
    "    Collects LinkedIn profile links from search results pages starting from the specified page. It iterates through the search results, extracts profile information, and handles pagination.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        page_start (int): The starting page number for scraping.\n",
    "        limit (int): The maximum number of pages to scrape.\n",
    "        total_results (int): The total number of search results.\n",
    "        unlimited (bool, optional): If True, ignores the page limit.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing the collected profile information.\n",
    "    \"\"\"\n",
    "\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            handle_retry_search(wd)\n",
    "\n",
    "            if page_start % 10 == 0:\n",
    "                print(f\"Scraping links on Page {page_start}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for person in people_list:\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info(person, names, curr_jobs, summarys, locations, links)\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "\n",
    "            # random_sleep(2, 3)\n",
    "            scroll_down(wd)\n",
    "\n",
    "            if not process_next_page(wd, page_start, limit, unlimited):\n",
    "                break\n",
    "\n",
    "            page_start += 1\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    })\n",
    "\n",
    "def handle_retry_search(wd):\n",
    "    \"\"\"\n",
    "    Handles the \"Retry Search\" scenario by waiting for the retry button to appear and clicking it after a random delay. This is useful for handling LinkedIn's anti-scraping measures.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        print(\"No 'Retry Search' button found, continuing with normal process.\")\n",
    "\n",
    "def collect_person_info(person, names, curr_jobs, summarys, locations, links):\n",
    "    \"\"\"\n",
    "    Collects information from a LinkedIn profile element and appends it to the respective lists for names, current jobs, summaries, locations, and profile links.\n",
    "\n",
    "    Args:\n",
    "        person (WebElement): The LinkedIn profile element to collect information from.\n",
    "        names (list): The list to store profile names.\n",
    "        curr_jobs (list): The list to store current job titles.\n",
    "        summarys (list): The list to store profile summaries.\n",
    "        locations (list): The list to store profile locations.\n",
    "        links (list): The list to store profile links.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "def extract_person_details(person, all_links):\n",
    "    \"\"\"\n",
    "    Extracts detailed information from a LinkedIn profile element, including name, current job, profile link, summary, and location.\n",
    "\n",
    "    Args:\n",
    "        person (WebElement): The LinkedIn profile element to extract information from.\n",
    "        all_links (list): The list of link elements within the profile element.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the extracted name, current job, profile link, summary, and location.\n",
    "    \"\"\"\n",
    "\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down(wd):\n",
    "    \"\"\"\n",
    "    Scrolls down the LinkedIn search results page to load more profiles for scraping.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page(wd, page_start, limit, unlimited):\n",
    "    \"\"\"\n",
    "    Navigates to the next page of LinkedIn search results if the \"Next\" button is enabled. It handles pagination logic based on the specified limit and the unlimited flag.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        page_start (int): The current page number being processed.\n",
    "        limit (int): The maximum number of pages to scrape.\n",
    "        unlimited (bool): If True, ignores the page limit.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if navigation to the next page was successful, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    elif not unlimited and page_start == limit + 1:\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def experience_json3(wd, link):\n",
    "    \"\"\"\n",
    "    Extracts the \"About\" section and experience details from a LinkedIn profile and returns them as a JSON-like dictionary.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        link (str): The LinkedIn profile link to extract information from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the \"About\" section and experience details.\n",
    "    \"\"\"\n",
    "\n",
    "    wd.get(link)\n",
    "    about_text = \"\"\n",
    "    WebDriverWait(wd, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))  # Ensure page is loaded\n",
    "\n",
    "    about_text = extract_about_section(wd)\n",
    "    experience_list = extract_experience_section(wd)\n",
    "\n",
    "    return jsonify(about_text, experience_list)\n",
    "\n",
    "def extract_about_section(wd):\n",
    "    \"\"\"\n",
    "    Extracts the \"About\" section text from a LinkedIn profile if it exists.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted \"About\" section text.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        about_tags = wd.find_elements(By.XPATH, \"//*[text()='About']\")\n",
    "        if about_tags:\n",
    "            about_tag = about_tags[0]\n",
    "            wd.execute_script(\"arguments[0].scrollIntoView();\", about_tag)\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid red'\", about_tag)\n",
    "            about_section_tag = about_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid blue'\", about_section_tag)\n",
    "            random_sleep(0, 1)\n",
    "            return about_section_tag.text.replace(\"About\\nAbout\\n\", \"\", 1)\n",
    "    except NoSuchElementException:\n",
    "        print(\"About section not found.\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_experience_section(wd):\n",
    "    \"\"\"\n",
    "    Extracts the experience details from a LinkedIn profile, including company names, job roles, and job durations.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing experience details.\n",
    "    \"\"\"\n",
    "\n",
    "    experience_list = []\n",
    "    try:\n",
    "        experience_tag = wd.find_element(By.XPATH, \"//*[text()='Experience']\")\n",
    "        wd.execute_script(\"arguments[0].scrollIntoView();\", experience_tag)\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", experience_tag)\n",
    "        section_tag = experience_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid blue'\", section_tag)\n",
    "        div_tag = section_tag.find_element(By.XPATH, \".//div[@class='pvs-list__outer-container']\")\n",
    "        jobs = div_tag.find_elements(By.XPATH, \"./ul/li\")\n",
    "\n",
    "        for job in jobs:\n",
    "            process_job_entry(job, experience_list)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Experience section not found.\")\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "def process_job_entry(job, experience_list):\n",
    "    \"\"\"\n",
    "    Processes a job entry from a LinkedIn profile and appends the extracted details to the experience list.\n",
    "\n",
    "    Args:\n",
    "        job (WebElement): The job entry element to process.\n",
    "        experience_list (list): The list to store experience details.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        company_name, job_role, job_time = extract_job_details(job)\n",
    "        if company_name:\n",
    "            experience_list.append({'company': company_name, 'job_role': job_role, 'job_time': job_time})\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to process job entry.\")\n",
    "\n",
    "def extract_job_details(job):\n",
    "    \"\"\"\n",
    "Extracts job details, including company name, job role, and job duration, from a job entry element.\n",
    "\n",
    "Args:\n",
    "    job (WebElement): The job entry element to extract details from.\n",
    "\n",
    "Returns:\n",
    "    tuple: A tuple containing the extracted company name, job role, and job duration.\n",
    "\"\"\"\n",
    "\n",
    "    company_name = job.find_element(By.CSS_SELECTOR, \"div.display-flex.flex-wrap.align-items-center.full-height span[aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_role = job.find_element(By.XPATH, \".//span[@aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_time = job.find_element(By.CSS_SELECTOR, \"span.t-14.t-normal.t-black--light span.pvs-entity__caption-wrapper\").text.split('·')[0].strip()\n",
    "    return company_name, job_role, job_time\n",
    "\n",
    "\n",
    "def jsonify(about_text, experience_list):\n",
    "    \"\"\"\n",
    "Converts the \"About\" section text and experience details into a JSON-like dictionary format.\n",
    "\n",
    "Args:\n",
    "    about_text (str): The \"About\" section text.\n",
    "    experience_list (list): The list of dictionaries containing experience details.\n",
    "\n",
    "Returns:\n",
    "    dict: A dictionary containing the \"About\" section and experience details.\n",
    "\"\"\"\n",
    "\n",
    "    final_dict = {\"About\": about_text, \"Experience\": {}}\n",
    "    for experience in experience_list:\n",
    "        company_dict = {}\n",
    "        if isinstance(experience['job_role'], list) and isinstance(experience['job_time'], list):\n",
    "            for role, time in zip(experience['job_role'], experience['job_time']):\n",
    "                company_dict[role] = time\n",
    "        else:\n",
    "            company_dict[experience['job_role']] = experience['job_time']\n",
    "        final_dict[\"Experience\"][experience['company']] = company_dict\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def get_search_results_number(wd):\n",
    "    \"\"\"\n",
    "Retrieves the number of search results from LinkedIn's search results page.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "Returns:\n",
    "    int: The number of search results.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        results_text = wd.find_element(By.CLASS_NAME, \"pb2.t-black--light.t-14\").text\n",
    "        # Assuming the format of results_text is either \"1,234 results\" or \"Showing 1-10 out of 1,234\"\n",
    "        number = int(results_text.split()[-2].replace(',', ''))\n",
    "        return number\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the search results element.\")\n",
    "        return 0\n",
    "    except ValueError:\n",
    "        print(\"Conversion error, possibly due to unexpected text format.\")\n",
    "        return 0\n",
    "\n",
    "def search_results_more_than_1000(wd):\n",
    "    \"\"\"\n",
    "Checks if the number of search results exceeds 1000.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "Returns:\n",
    "    bool: True if the number of search results is more than 1000, False otherwise.\n",
    "\"\"\"\n",
    "\n",
    "    number = get_search_results_number(wd)\n",
    "    return number > 1000\n",
    "\n",
    "\n",
    "def dataframe_output(wd, search_term, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False, industry=True):\n",
    "    \"\"\"\n",
    "    Conducts a search based on specified parameters and collects results into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance.\n",
    "    search_term (str): The search query.\n",
    "    location (str): Filter by location.\n",
    "    current_company (str): Filter by current company.\n",
    "    past_company (str): Filter by past company.\n",
    "    num_pages (int): Number of pages to scrape.\n",
    "    unlimited (bool): If True, ignores page limits.\n",
    "    industry (bool): If True, applies industry filters if results exceed 1000.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A DataFrame containing the collected data and a list of profile links.\n",
    "    \"\"\"\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    try:\n",
    "        search_query(wd, search_term)  # Assuming search_query now accepts wd as a parameter\n",
    "        \n",
    "        filters_page = add_filters1(wd, location, current_company, past_company)\n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            for i in industry_options:\n",
    "                industry_filter(wd, i)\n",
    "                total_results = get_search_results_number(wd)\n",
    "                sub_df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['Profile Link'])\n",
    "                wd.get(filters_page)\n",
    "        else:\n",
    "            total_results = get_search_results_number(wd)\n",
    "            df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        print(\"Exporting current dataframe...\")\n",
    "\n",
    "    links = list(df['Profile Link'])\n",
    "    return df, links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process(wd, keywords, companies, username, password):\n",
    "    \"\"\"\n",
    "    This function performs the main scraping process on LinkedIn profiles based on given keywords and companies.\n",
    "    It logs into LinkedIn, iterates through each keyword and company pair, and scrapes relevant profile information.\n",
    "    The scraped data is stored in a DataFrame and saved to CSV files. The function handles any exceptions by saving\n",
    "    the current state of the DataFrame before stopping execution.\n",
    "\n",
    "    Args:\n",
    "    wd: WebDriver instance for browser automation.\n",
    "    keywords (list): A list of keywords to search for in LinkedIn profiles.\n",
    "    companies (list): A list of companies to search within LinkedIn profiles.\n",
    "    username (str): LinkedIn account username.\n",
    "    password (str): LinkedIn account password.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    login(wd, username, password)\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link', 'Keyword', 'Company']\n",
    "    job_df = pd.DataFrame(columns=columns)\n",
    "    keyword = \"\"\n",
    "    company = \"\"\n",
    "    query = \"\"\n",
    "\n",
    "    try:\n",
    "        for keyword in keywords:\n",
    "            timeTotal = 0\n",
    "            for i in range(0, len(companies) - 2, 2):\n",
    "\n",
    "                if i != 0:    \n",
    "                    wd.close()\n",
    "                    print(\"Sleeping for 2 minutes\")\n",
    "                    random_sleep(100, 120)\n",
    "                    wd = refresh_wd()\n",
    "\n",
    "                login(wd, username, password)\n",
    "                for company in companies[i:i+2]:\n",
    "                    query = keyword\n",
    "                    location = \"United States\"\n",
    "                    current_company = company\n",
    "                    past_company = None\n",
    "                    unlimited = False\n",
    "                    num_pages = 5\n",
    "                    print(\"Scraping your request...\")\n",
    "                    print((query, current_company))\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    df, links = dataframe_output(wd, query, location, current_company, past_company, num_pages, unlimited, industry=True)\n",
    "                    print(len(df))\n",
    "                    end_time = time.time()\n",
    "                    timeTotal += (end_time - start_time)\n",
    "                    print(end_time - start_time)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        df['Keyword'] = keyword\n",
    "                        df['Company'] = current_company\n",
    "                        df.to_csv(f\"{query.replace(' ', '')}_CompanyDFs/{company.replace(' ', '')}.csv\")\n",
    "                        job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "\n",
    "                    job_df.to_csv(f\"{query.replace(' ', '')}_profiles.csv\")\n",
    "                    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "            print(f\"Average Time per Company Scrape: {(timeTotal / len(companies)) / 60} minutes\")\n",
    "            print(f\"Total Time per Company Scrape: {timeTotal / 60} minutes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if len(df) > 0:\n",
    "            df['Keyword'] = keyword\n",
    "            df['Company'] = company\n",
    "            job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "        job_df.to_csv(f\"{query.replace(' ', '')}_profiles_er.csv\")\n",
    "        print(f'An error occurred: {str(e)} and execution stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "username, password = None, None\n",
    "\n",
    "def set_user_as_PurdueCS(Purdue=True):\n",
    "    global username, password\n",
    "    if Purdue:\n",
    "        username = \"huoerxiu@gmail.com\"\n",
    "        password = \"PURDUEcs\"\n",
    "    else:\n",
    "        username = \"bhat35@purdue.edu\"\n",
    "        password = \"Aymwos@1977!!\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "wd = webdriver.Chrome(options=options)\n",
    "\n",
    "# wd = webdriver.Chrome(ChromeDriverManager().install())\n",
    "actions = ActionChains(wd)\n",
    "\n",
    "wd.maximize_window()\n",
    "wd.switch_to.window(wd.current_window_handle)\n",
    "wd.implicitly_wait(10)\n",
    "\n",
    "wd.get('https://www.linkedin.com/')\n",
    "\n",
    "wd.implicitly_wait(1)\n",
    "\n",
    "keywords = ['Machine Learning Engineer', \"Artificial Intelligence\", \"AI\", \"Machine Learning\", \"ML\", \"Natural Language Processing\", \"NLP\", \"Data Mining\", \n",
    "    \"Robotics\", \"Computer Vision\", \"CV\", \"Autonomous Driving\", \"Self-driving\", \"Medical Imaging\", \"object detection\", \n",
    "    \"disease prediction\", \"disease diagnosis\", \"Data Science\", \"Deep Learning\", \"DL\", \"Language Modeling\", \"Language Models\", \n",
    "    \"Data Analysis\", \"Information Retrieval\", \"Reinforcement Learning\", \"Data Scientist\", \"Data Scientists\", \"Knowledge Graph\", \n",
    "    \"Knowledge Representation\", \"Representation Learning\", \"Generative Models\", \"Graph Mining\", \"Graph Model\", \"Perception Model\", \n",
    "    \"Predictive Model\", \"Predictive Diagnosis\", \"Model Prediction\", \"Model Robustness\", \"Neural Network\", \"Neural Model\", \"DNN\", \n",
    "    \"Bayesian Network\", \"Bayesian Model\", \"SVM\", \"PCA\", \"LASSO\", \"Transformer Model\", \"ChatGPT\", \"Pre-trained Model\", \"GPT\", \n",
    "    \"Named Entity Recognition\", \"Sentiment Analysis\", \"Image Segmentation\", \"Relation Extraction\", \"Information Extraction\", \n",
    "    \"Text Generation\", \"Conversational Agent\", \"Autonomous Agent\", \"Autonomous System\", \"Robotic Vehicle\", \"Unmanned Vehicle\", \n",
    "    \"Unmanned Aerial Vehicle\", \"UVA\", \"Unmanned Ground Vehicle\", \"Image Generation\", \"Diffusion Model\", \"Stable Diffusion\", \n",
    "    \"QA Model\", \"Intelligent System\", \"Intelligent Engine\", \"Information Retrieval\", \"Machine Intelligence\", \"CNN\", \"ResNet\", \n",
    "    \"VGG\", \"LeNet\", \"AlexNet\", \"Yolo\", \"DenseNet\", \"Vision Transformer\", \"Language Transformer\", \"Text Transformer\", \n",
    "    \"Semantic Parsing\", \"DeepViT\", \"Percepton\", \"MLP\", \"Encoder-Decoder\", \"AutoEncoder\", \"Variational Encoder\", \"GAN\", \n",
    "    \"Adversarial Network\", \"VAE\", \"Data Labeling\", \"Data Augmentation\", \"Semantic Segmentation\", \"Feature Extraction\", \n",
    "    \"Feature Engineering\", \"Prompt Engineering\", \"BERT\", \"RNN\", \"CLIP\", \"Video Generation\", \"Face Recognition\", \"Object Tracking\", \n",
    "    \"Machine Translation\", \"Text Translation\", \"Sequence-to-Sequence Model\", \"Seq-to-Seq Model\", \"Image-to-Image Translation\", \n",
    "    \"Image Translation\", \"Action Recognition\", \"Movement Recognition\", \"Object Segmentation\", \"Video Recognition\", \"Pose Estimation\", \n",
    "    \"Depth Estimation\", \"Image Retrieval\", \"Document Retrieval\", \"Text Retrieval\", \"Model Training\", \"Adversarial Training\", \n",
    "    \"Adversarial Attack\", \"Model Safety\", \"Model Security\", \"Supervised Learning\", \"Unsupervised Learning\", \"Self-supervised Learning\", \n",
    "    \"Generative Training\", \"Weak Supervision\", \"Data Annotation\", \"Explainable Model\", \"Style Transfer\", \"Interpretable Model\", \n",
    "    \"Model Explanability\", \"Model Interpretability\", \"Upsamplng\", \"Downsampling\", \"text representation\", \"video representation\", \n",
    "    \"image representation\", \"VQA\", \"visual grounding\", \"image transformation\", \"inpainting\", \"trajectory prediction\", \n",
    "    \"motion detection\", \"motion prediction\", \"posture detection\", \"posture recognition\"]\n",
    "companies = ['Netflix', \n",
    "             'Amazon',\n",
    "             'Apple', \n",
    "             'Meta', \n",
    "             'Google', \n",
    "             'Microsoft', \n",
    "             'OpenAI',\n",
    "             'Intel',\n",
    "             'Cicsco',\n",
    "             'NVIDIA',\n",
    "             'Salesforce',\n",
    "             'LinkedIn',\n",
    "             'DeepMind',\n",
    "             'IBM',\n",
    "             'Bloomberg',\n",
    "             'Tesla',\n",
    "             'Mayo Clinic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_total_results(message):\n",
    "    file_path = 'total_results_per_search.txt'\n",
    "\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def collect_links1(wd, search_term, current_company, state, limit, total_results, unlimited=False, output_file='continuous_data_scrape.csv'):\n",
    "    \"\"\"\n",
    "Collects LinkedIn profile links from search results starting from the specified page and continues until the limit or total results are reached. It saves the collected data to a CSV file and updates the scraping state.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "    search_term (str): The search query to be entered.\n",
    "    current_company (str): The company to filter profiles by.\n",
    "    state (dict): The current state of the scraping process, including the starting page and other parameters.\n",
    "    limit (int): The maximum number of pages to scrape.\n",
    "    total_results (int): The total number of search results.\n",
    "    unlimited (bool, optional): If True, ignores the page limit and continues scraping until all results are collected.\n",
    "    output_file (str, optional): The file to save the collected data. Default is 'continuous_data_scrape.csv'.\n",
    "\n",
    "Returns:\n",
    "    DataFrame: A pandas DataFrame containing the collected profile information.\n",
    "\"\"\"\n",
    "\n",
    "    state = load_state()\n",
    "    page_to_start_at = state['page_start']\n",
    "\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "    print(\"Unlimited:\", unlimited)\n",
    "\n",
    "    ### IMPLEMENT THE BELOW METHOD\n",
    "    if page_to_start_at != 1:\n",
    "        if not go_to_page(wd, page_to_start_at):\n",
    "            print(f\"Could not navigate to page {page_to_start_at}, trying again...\")\n",
    "            if not go_to_page(wd, page_to_start_at):\n",
    "                print(f\"Failed to navigate to page {page_to_start_at}, aborting...\")\n",
    "                return pd.DataFrame()  # Fail gracefully\n",
    "\n",
    "    curr_page = page_to_start_at\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            p_names, p_curr_jobs, p_summarys, p_locations, p_links = [], [], [], [], []\n",
    "            handle_retry_search1(wd)\n",
    "\n",
    "            if curr_page % 10 == 0:\n",
    "                print(f\"Scraping links on Page {curr_page}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for index, person in enumerate(people_list):\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info1(person, names, curr_jobs, summarys, locations, links) # Adding to general lists\n",
    "                    collect_person_info1(person, p_names, p_curr_jobs, p_summarys, p_locations, p_links) # Adding to lists of specific page\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'Name': p_names,\n",
    "                'Current Job': p_curr_jobs,\n",
    "                'Relevant Experience to Keyword': p_summarys,\n",
    "                'Location': p_locations,\n",
    "                'Profile Link': p_links\n",
    "            })\n",
    "\n",
    "            df['Keyword'] = search_term\n",
    "            df['Company'] = current_company\n",
    "\n",
    "            if not os.path.isfile(output_file):\n",
    "                df.to_csv(output_file, index=False)  # Create file and write data\n",
    "            else:\n",
    "                df.to_csv(output_file, mode='a', header=False, index=False)  # Append data to existing file\n",
    "\n",
    "            scroll_down1(wd)\n",
    "            if not process_next_page1(wd, curr_page, limit, unlimited):\n",
    "                break\n",
    "\n",
    "            curr_page += 1\n",
    "            state.update({'page_start': curr_page})\n",
    "            save_state(state)\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "        # Handling to save the state or decide what to do next can be added here\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    })\n",
    "\n",
    "def go_to_page(wd, page_number):\n",
    "    \"\"\"\n",
    "Navigates to the specified page number in the LinkedIn search results. If the desired page number is not visible, it uses ellipsis buttons to load more pages.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "    page_number (int): The page number to navigate to.\n",
    "\n",
    "Returns:\n",
    "    bool: True if navigation to the specified page was successful, False otherwise.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Wait for the pagination container to be visible\n",
    "        scroll_down1(wd)\n",
    "        pagination = WebDriverWait(wd, 5).until(\n",
    "            EC.visibility_of_element_located((By.CLASS_NAME, 'artdeco-pagination__pages'))\n",
    "        )\n",
    "        while True:\n",
    "            pages = pagination.find_elements(By.TAG_NAME, 'button')\n",
    "            # Attempt to find and click the desired page number\n",
    "            page_found = False\n",
    "            ellipses = []\n",
    "            for page in pages:\n",
    "                if '…' in page.text:\n",
    "                    ellipses.append(page)\n",
    "\n",
    "                if page.text.strip() == str(page_number):\n",
    "                    page.click()\n",
    "                    return True\n",
    "\n",
    "            # If not found, find and click the \"...\" button to load more pages\n",
    "            # ellipses = [btn for btn in pages if \"...\" in btn.text]\n",
    "            if not ellipses:\n",
    "                print(f\"No '…'found and page {page_number} is not present.\")\n",
    "                return False\n",
    "\n",
    "            # Click the appropriate \"...\" button\n",
    "            # If looking for a higher page number, click the last \"...\" button, else the first\n",
    "            target_ellipsis = ellipses[-1] if page_number > int(pages[-3].text.strip()) else ellipses[0]\n",
    "            target_ellipsis.click()\n",
    "            WebDriverWait(wd, 3).until(EC.staleness_of(target_ellipsis))  # Ensure the pagination has refreshed\n",
    "\n",
    "            # Reassign pagination element to handle potential StaleElementReferenceException\n",
    "            pagination = WebDriverWait(wd, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'artdeco-pagination__pages'))\n",
    "            )\n",
    "    except TimeoutException as e:\n",
    "        print(f\"Failed to navigate to page {page_number}: {e}\")\n",
    "        return False\n",
    "\n",
    "def handle_retry_search1(wd):\n",
    "    \"\"\"\n",
    "Handles the \"Retry Search\" scenario by waiting for the retry button to appear and clicking it after a random delay. This helps to bypass LinkedIn's anti-scraping measures.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 1).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        not_found = True\n",
    "\n",
    "def collect_person_info1(person, names, curr_jobs, summarys, locations, links):\n",
    "    \"\"\"\n",
    "Collects information from a LinkedIn profile element and appends it to the respective lists for names, current jobs, summaries, locations, and profile links.\n",
    "\n",
    "Args:\n",
    "    person (WebElement): The LinkedIn profile element to collect information from.\n",
    "    names (list): The list to store profile names.\n",
    "    curr_jobs (list): The list to store current job titles.\n",
    "    summarys (list): The list to store profile summaries.\n",
    "    locations (list): The list to store profile locations.\n",
    "    links (list): The list to store profile links.\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details1(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "    \n",
    "\n",
    "def extract_person_details1(person, all_links):\n",
    "    \"\"\"\n",
    "Extracts detailed information from a LinkedIn profile element, including name, current job, profile link, summary, and location.\n",
    "\n",
    "Args:\n",
    "    person (WebElement): The LinkedIn profile element to extract information from.\n",
    "    all_links (list): The list of link elements within the profile element.\n",
    "\n",
    "Returns:\n",
    "    tuple: A tuple containing the extracted name, current job, profile link, summary, and location.\n",
    "\"\"\"\n",
    "\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down1(wd):\n",
    "    \"\"\"\n",
    "Scrolls down the LinkedIn search results page to load more profiles for scraping.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page1(wd, curr_page, limit, unlimited):\n",
    "    \"\"\"\n",
    "Navigates to the next page of LinkedIn search results if the \"Next\" button is enabled. It handles pagination logic based on the specified limit and the unlimited flag.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "    curr_page (int): The current page number being processed.\n",
    "    limit (int): The maximum number of pages to scrape.\n",
    "    unlimited (bool): If True, ignores the page limit.\n",
    "\n",
    "Returns:\n",
    "    bool: True if navigation to the next page was successful, False otherwise.\n",
    "\"\"\"\n",
    "\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if not unlimited and curr_page == limit:\n",
    "        return False\n",
    "\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_state(state):\n",
    "    \"\"\"\n",
    "Saves the current state of the scraping process to a JSON file for persistence across sessions.\n",
    "\n",
    "Args:\n",
    "    state (dict): The state of the scraping process to be saved.\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "\n",
    "    with open('state.json', 'w') as f:\n",
    "        json.dump(state, f)\n",
    "\n",
    "def load_state():\n",
    "    \"\"\"\n",
    "    Loads the saved state of the scraping process from a JSON file. If the file does not exist, returns None.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        dict: The state of the scraping process, or None if the state file does not exist.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open('state.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def dataframe_output1(wd, search_term, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False):\n",
    "    \"\"\"\n",
    "    Conducts a search based on specified parameters and collects results into a DataFrame. Applies filters and collects profile information from multiple pages of search results, updating the state throughout the process.\n",
    "\n",
    "    Args:\n",
    "        wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "        search_term (str): The search query.\n",
    "        location (str, optional): Filter by location.\n",
    "        current_company (str, optional): Filter by current company.\n",
    "        past_company (str, optional): Filter by past company.\n",
    "        num_pages (int, optional): Number of pages to scrape. Default is 4.\n",
    "        unlimited (bool, optional): If True, ignores page limits. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A pandas DataFrame containing the collected profile information.\n",
    "    \"\"\"\n",
    "\n",
    "    state = load_state()\n",
    "    industry = state['industry']\n",
    "\n",
    "    df = pd.DataFrame(columns=['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link'])\n",
    "    try:\n",
    "        search_query(wd, search_term)\n",
    "        # add_filters1(wd, location, current_company, past_company)\n",
    "        \n",
    "        filters_page = add_filters1(wd, location, current_company, past_company)\n",
    "        print(filters_page)\n",
    "\n",
    "        total_results = get_search_results_number(wd)\n",
    "        total_results_string = f'{keywords[state[\"keyword_index\"]]}-{companies[state[\"company_index\"]]}: {total_results}'\n",
    "        write_total_results(total_results_string)\n",
    "\n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            industry_options = industry_options[state['industry_number']:]\n",
    "\n",
    "            for i in industry_options:\n",
    "\n",
    "                industry_filter(wd, i)\n",
    "                sub_df = collect_links1(wd, search_term, current_company, state, num_pages, total_results, unlimited)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates()\n",
    "                print(filters_page)\n",
    "                wd.get(filters_page)\n",
    "                random_sleep(1,2)\n",
    "                \n",
    "                state = load_state()\n",
    "                state.update({'page_start': 1, 'industry_number': ((i+1)%5)})\n",
    "                save_state(state)\n",
    "        else:\n",
    "            df = collect_links1(wd, search_term, current_company, state, num_pages, total_results, unlimited)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        # Here you might want to save the last state or rethrow the exception to handle it in the main scraping function\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process_rob(wd, keywords, companies, username, password, output_file='scraped_data.csv'):\n",
    "    \"\"\"\n",
    "The main robust scraping function that takes in exceptions and restarts the process when an issue occurs. It logs into LinkedIn, iterates through each keyword and company pair, scrapes relevant profile information, and handles the state of the scraping process to resume from where it left off.\n",
    "\n",
    "Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance used for browser automation.\n",
    "    keywords (list): A list of keywords to search for in LinkedIn profiles.\n",
    "    companies (list): A list of companies to search within LinkedIn profiles.\n",
    "    username (str): LinkedIn account username.\n",
    "    password (str): LinkedIn account password.\n",
    "    output_file (str, optional): The file to save the collected data. Default is 'scraped_data.csv'.\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "\n",
    "    state = load_state()\n",
    "    if state is None:\n",
    "        state = {'keyword_index': 0, 'company_index': 0, 'page_start': 1, 'industry' : True ,'industry_number': 0, 'problem_locs': {}}\n",
    "        save_state(state)\n",
    "\n",
    "    try:\n",
    "        keyword_start_index = state['keyword_index']\n",
    "        company_start_index = state['company_index']\n",
    "        for keyword_index in range(keyword_start_index, len(keywords)):\n",
    "            keyword = keywords[keyword_index]\n",
    "            \n",
    "            if keyword_index != keyword_start_index:\n",
    "                state = load_state()\n",
    "                state.update({'keyword_index': keyword_index, 'company_index': 0, 'page_start': 1, 'industry_number': 0})\n",
    "                save_state(state)\n",
    "                company_start_index = state['company_index']\n",
    "\n",
    "            for company_index in range(company_start_index, len(companies)):\n",
    "                company = companies[company_index]\n",
    "                \n",
    "                if company_index != company_start_index:\n",
    "                    state = load_state()\n",
    "                    state.update({'company_index': company_index, 'page_start': 1, 'industry_number': 0})\n",
    "                    save_state(state)\n",
    "\n",
    "                try:\n",
    "                    print(f\"Scraping your request for {keyword} at {company}\")\n",
    "                    df = dataframe_output1(\n",
    "                        wd=wd, search_term=keyword, location=\"United States\", current_company=company, past_company=None, num_pages=10, unlimited=True)\n",
    "                    \n",
    "                    df['Keyword'] = keyword\n",
    "                    df['Company'] = company\n",
    "                    if not os.path.isfile(output_file):\n",
    "                        df.to_csv(output_file, index=False)  # Create file and write data\n",
    "                    else:\n",
    "                        df.to_csv(output_file, mode='a', header=False, index=False)  # Append data to existing file\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Encountered an issue with {company}: {e}, saving state and restarting...\")\n",
    "                    state = load_state()\n",
    "\n",
    "                    problem_locs = state['problem_locs']\n",
    "                    keyword_index_str = str(keyword_index)\n",
    "                    current_list = problem_locs.get(keyword_index_str, [])\n",
    "                    current_list.append(f\"{company_index}.{state['page_start']}.{state['industry_number']}\")\n",
    "                    problem_locs[keyword_index_str] = current_list\n",
    "\n",
    "                    state.update({'problem_locs':problem_locs})\n",
    "                    save_state(state)\n",
    "\n",
    "                wd.get('https://www.linkedin.com/')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Final error occurred: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huoerxiu@gmail.com PURDUEcs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to login: Timeout while waiting for page elements.\n"
     ]
    }
   ],
   "source": [
    "set_user_as_PurdueCS(Purdue=True)\n",
    "print(username,password)\n",
    "\n",
    "login(wd, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping your request for AI at Tesla\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%2215564%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=ai&origin=FACETED_SEARCH&sid=bZw\n",
      "Added industry option 2\n",
      "Unlimited: True\n",
      "Failed to navigate to page 29: Message: \n",
      "Stacktrace:\n",
      "#0 0x588ef075c993 <unknown>\n",
      "#1 0x588ef0457136 <unknown>\n",
      "#2 0x588ef04a1d48 <unknown>\n",
      "#3 0x588ef04a1e01 <unknown>\n",
      "#4 0x588ef04e4e44 <unknown>\n",
      "#5 0x588ef04c3cfd <unknown>\n",
      "#6 0x588ef04e2319 <unknown>\n",
      "#7 0x588ef04c3a73 <unknown>\n",
      "#8 0x588ef0494c93 <unknown>\n",
      "#9 0x588ef049565e <unknown>\n",
      "#10 0x588ef072108b <unknown>\n",
      "#11 0x588ef0725005 <unknown>\n",
      "#12 0x588ef070f491 <unknown>\n",
      "#13 0x588ef0725b92 <unknown>\n",
      "#14 0x588ef06f49ef <unknown>\n",
      "#15 0x588ef074bdf8 <unknown>\n",
      "#16 0x588ef074bfcb <unknown>\n",
      "#17 0x588ef075bae4 <unknown>\n",
      "#18 0x7a274ee94ac3 <unknown>\n",
      "\n",
      "Could not navigate to page 29, trying again...\n",
      "Scraping links on Page 30\n"
     ]
    }
   ],
   "source": [
    "# main_scraping_process(wd, keywords, companies, username, password)846448\n",
    "main_scraping_process_rob(wd, keywords, companies, username, password)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be8df3a746dc770fc8ec50a8a16d0a923a6575bbbcf271bc91914f68c2488458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
