{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def random_sleep(start, end):\n",
    "    time.sleep(random.randint(start, end))\n",
    "\n",
    "def refresh_wd():\n",
    "    # Define your Chrome options outside this function or ensure they are passed as parameters\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    # Create a new instance of Chrome\n",
    "    wd = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Initialize action chains for possible future actions\n",
    "    actions = ActionChains(wd)\n",
    "    \n",
    "    # Maximize the window to avoid elements being out of view\n",
    "    wd.maximize_window()\n",
    "    \n",
    "    # Navigate to LinkedIn\n",
    "    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "    return wd\n",
    "\n",
    "def login(wd, user_name, password):\n",
    "    try:\n",
    "        # Use WebDriverWait for more reliable element handling\n",
    "        username_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_key\"))\n",
    "        )\n",
    "        username_input.clear()\n",
    "        username_input.send_keys(user_name)\n",
    "        \n",
    "        password_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"session_password\"))\n",
    "        )\n",
    "        password_input.clear()\n",
    "        password_input.send_keys(password)\n",
    "\n",
    "        print(\"Logging into your LinkedIn account!\")\n",
    "        \n",
    "        # Locate and click the login button\n",
    "        login_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/button'))\n",
    "        )\n",
    "        login_button.click()\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to login: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to login: Could not find one of the elements.\")\n",
    "\n",
    "def security_verification(wd):\n",
    "    otp = input(\"You have been sent a verification code from LinkedIn via your email.\\nPlease input that here: \")\n",
    "    try:\n",
    "        # Use WebDriverWait to wait for the OTP input field to become available\n",
    "        otp_input = WebDriverWait(wd, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div/main/form/div[1]/input[15]'))\n",
    "        )\n",
    "        otp_input.clear()\n",
    "        otp_input.send_keys(otp)\n",
    "\n",
    "        # Locate and click the submit button for OTP\n",
    "        submit_button = WebDriverWait(wd, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/div/main/form/div[2]/button'))\n",
    "        )\n",
    "        submit_button.click()\n",
    "        print(\"Successfully Authenticated!\")\n",
    "    except TimeoutException:\n",
    "        print(\"Authentication failed: Timeout while waiting for page elements.\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Authentication failed: Could not find one of the elements.\")\n",
    "\n",
    "def search_query(wd, query):\n",
    "    # Ensure the search input field is visible and clickable\n",
    "    search_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, 'search-global-typeahead__input'))\n",
    "    )\n",
    "    search_field.click()  # Focus on the search field\n",
    "\n",
    "    # Introduce a random delay to mimic human typing speed\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Send the search query\n",
    "    search_field.send_keys(query)\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    # Press ENTER to execute the search\n",
    "    search_field.send_keys(Keys.ENTER)\n",
    "\n",
    "\n",
    "def add_filters1(wd, location=None, current_company=None, past_company=None):\n",
    "    navigate_to_people_results(wd)\n",
    "    open_all_filters(wd)\n",
    "\n",
    "    if current_company:\n",
    "        apply_filter(wd, current_company, 'company', filter_index=0)\n",
    "    if past_company:\n",
    "        apply_filter(wd, past_company, 'company', filter_index=1)\n",
    "    if location:\n",
    "        apply_filter(wd, location, 'location')\n",
    "\n",
    "    show_all_results(wd)\n",
    "    random_sleep(1,2)\n",
    "\n",
    "    url = wd.current_url\n",
    "    return url\n",
    "\n",
    "def apply_filter(wd, filter_value, filter_type, filter_index=None):\n",
    "    # Locate the \"Add a filter\" button based on the type and index (if applicable)\n",
    "    filter_button_xpath = f\"//*[text()='Add a {filter_type}']\"\n",
    "    filter_buttons = wd.find_elements(By.XPATH, filter_button_xpath)\n",
    "    filter_button = filter_buttons[filter_index] if filter_index is not None else filter_buttons[0]\n",
    "\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", filter_button)\n",
    "    filter_button.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Enter the filter value in the corresponding input field\n",
    "    input_selector = f'input[placeholder=\"Add a {filter_type}\"]'\n",
    "    input_field = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, input_selector))\n",
    "    )\n",
    "    input_field.click()\n",
    "    input_field.send_keys(filter_value)\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "    # Wait for the listbox to appear and select the correct option\n",
    "    listbox = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, 'basic-typeahead__triggered-content'))\n",
    "    )\n",
    "    options = listbox.find_elements(By.XPATH, \".//div[@role='option']\")\n",
    "    for option in options:\n",
    "        if filter_value in option.text:\n",
    "            option.click()\n",
    "            break\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def navigate_to_people_results(wd):\n",
    "    try:\n",
    "        # Use WebDriverWait to wait until the button is visible and clickable\n",
    "        people_results_button = WebDriverWait(wd, 5).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'See all people results')]\"))\n",
    "        )\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", people_results_button)\n",
    "        people_page_link = people_results_button.get_attribute(\"href\")\n",
    "        wd.get(people_page_link)\n",
    "        # Wait for a random time between 1 and 2 seconds after loading the page\n",
    "        random_sleep(1, 2)\n",
    "    except TimeoutException:\n",
    "        print(\"Failed to find the 'See all people results' button within the expected time.\")\n",
    "\n",
    "\n",
    "def open_all_filters(wd):\n",
    "    random_sleep(2,3)\n",
    "    all_filters = wd.find_element(By.CLASS_NAME, \"relative.mr2\")\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    random_sleep(1,2)\n",
    "\n",
    "\n",
    "def industry_filter(wd, industry_list_num=0):\n",
    "    # Navigate and click on all filters button\n",
    "    navigate_and_click_filters(wd)\n",
    "\n",
    "    # Scroll to and interact with the industry filter\n",
    "    industry_filter_ul = WebDriverWait(wd, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[text()='Add an industry']/ancestor::ul[1]\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].scrollIntoView();\", industry_filter_ul)\n",
    "\n",
    "    # Select the industry based on provided index\n",
    "    select_industry(wd, industry_filter_ul, industry_list_num)\n",
    "\n",
    "    # Click show all results\n",
    "    show_all_results(wd)\n",
    "\n",
    "def navigate_and_click_filters(wd):\n",
    "    all_filters = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, \"relative.mr2\"))\n",
    "    )\n",
    "    wd.execute_script(\"arguments[0].style.border='2px solid red'\", all_filters)\n",
    "    all_filters.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "def select_industry(wd, industry_filter_ul, industry_list_num):\n",
    "    li_elements = industry_filter_ul.find_elements(By.TAG_NAME, \"li\")\n",
    "    if 0 <= industry_list_num < len(li_elements):\n",
    "        li_element = li_elements[industry_list_num]\n",
    "        input_checkbox = li_element.find_element(By.XPATH, \".//input[@type='checkbox']\")\n",
    "        if not input_checkbox.is_selected():\n",
    "            wd.execute_script(\"arguments[0].click();\", input_checkbox)\n",
    "        print(f\"Added industry option {industry_list_num}\")\n",
    "    else:\n",
    "        print(\"Invalid industry index\")\n",
    "\n",
    "def show_all_results(wd):\n",
    "    all_results = WebDriverWait(wd, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/div[3]/div/div/div[3]/div/button[2]'))\n",
    "    )\n",
    "    all_results.click()\n",
    "    time.sleep(random.randint(1, 2))\n",
    "\n",
    "\n",
    "def collect_links(wd, page_start, limit, total_results, unlimited=False):\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            handle_retry_search(wd)\n",
    "\n",
    "            if page_start % 10 == 0:\n",
    "                print(f\"Scraping links on Page {page_start}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for person in people_list:\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info(person, names, curr_jobs, summarys, locations, links)\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "\n",
    "            # random_sleep(2, 3)\n",
    "            scroll_down(wd)\n",
    "\n",
    "            if not process_next_page(wd, page_start, limit, unlimited):\n",
    "                break\n",
    "\n",
    "            page_start += 1\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    })\n",
    "\n",
    "def handle_retry_search(wd):\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        print(\"No 'Retry Search' button found, continuing with normal process.\")\n",
    "\n",
    "def collect_person_info(person, names, curr_jobs, summarys, locations, links):\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "def extract_person_details(person, all_links):\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down(wd):\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page(wd, page_start, limit, unlimited):\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    elif not unlimited and page_start == limit + 1:\n",
    "        return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def experience_json3(wd, link):\n",
    "    wd.get(link)\n",
    "    about_text = \"\"\n",
    "    WebDriverWait(wd, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))  # Ensure page is loaded\n",
    "\n",
    "    about_text = extract_about_section(wd)\n",
    "    experience_list = extract_experience_section(wd)\n",
    "\n",
    "    return jsonify(about_text, experience_list)\n",
    "\n",
    "def extract_about_section(wd):\n",
    "    try:\n",
    "        about_tags = wd.find_elements(By.XPATH, \"//*[text()='About']\")\n",
    "        if about_tags:\n",
    "            about_tag = about_tags[0]\n",
    "            wd.execute_script(\"arguments[0].scrollIntoView();\", about_tag)\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid red'\", about_tag)\n",
    "            about_section_tag = about_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "            wd.execute_script(\"arguments[0].style.border='2px solid blue'\", about_section_tag)\n",
    "            random_sleep(0, 1)\n",
    "            return about_section_tag.text.replace(\"About\\nAbout\\n\", \"\", 1)\n",
    "    except NoSuchElementException:\n",
    "        print(\"About section not found.\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_experience_section(wd):\n",
    "    experience_list = []\n",
    "    try:\n",
    "        experience_tag = wd.find_element(By.XPATH, \"//*[text()='Experience']\")\n",
    "        wd.execute_script(\"arguments[0].scrollIntoView();\", experience_tag)\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid red'\", experience_tag)\n",
    "        section_tag = experience_tag.find_element(By.XPATH, \"ancestor::section\")\n",
    "        wd.execute_script(\"arguments[0].style.border='2px solid blue'\", section_tag)\n",
    "        div_tag = section_tag.find_element(By.XPATH, \".//div[@class='pvs-list__outer-container']\")\n",
    "        jobs = div_tag.find_elements(By.XPATH, \"./ul/li\")\n",
    "\n",
    "        for job in jobs:\n",
    "            process_job_entry(job, experience_list)\n",
    "    except NoSuchElementException:\n",
    "        print(\"Experience section not found.\")\n",
    "\n",
    "    return experience_list\n",
    "\n",
    "def process_job_entry(job, experience_list):\n",
    "    try:\n",
    "        company_name, job_role, job_time = extract_job_details(job)\n",
    "        if company_name:\n",
    "            experience_list.append({'company': company_name, 'job_role': job_role, 'job_time': job_time})\n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to process job entry.\")\n",
    "\n",
    "def extract_job_details(job):\n",
    "    company_name = job.find_element(By.CSS_SELECTOR, \"div.display-flex.flex-wrap.align-items-center.full-height span[aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_role = job.find_element(By.XPATH, \".//span[@aria-hidden='true']\").text.split('·')[0].strip()\n",
    "    job_time = job.find_element(By.CSS_SELECTOR, \"span.t-14.t-normal.t-black--light span.pvs-entity__caption-wrapper\").text.split('·')[0].strip()\n",
    "    return company_name, job_role, job_time\n",
    "\n",
    "\n",
    "def jsonify(about_text, experience_list):\n",
    "    final_dict = {\"About\": about_text, \"Experience\": {}}\n",
    "    for experience in experience_list:\n",
    "        company_dict = {}\n",
    "        if isinstance(experience['job_role'], list) and isinstance(experience['job_time'], list):\n",
    "            for role, time in zip(experience['job_role'], experience['job_time']):\n",
    "                company_dict[role] = time\n",
    "        else:\n",
    "            company_dict[experience['job_role']] = experience['job_time']\n",
    "        final_dict[\"Experience\"][experience['company']] = company_dict\n",
    "    return final_dict\n",
    "\n",
    "\n",
    "def get_search_results_number(wd):\n",
    "    try:\n",
    "        results_text = wd.find_element(By.CLASS_NAME, \"pb2.t-black--light.t-14\").text\n",
    "        # Assuming the format of results_text is either \"1,234 results\" or \"Showing 1-10 out of 1,234\"\n",
    "        number = int(results_text.split()[-2].replace(',', ''))\n",
    "        return number\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find the search results element.\")\n",
    "        return 0\n",
    "    except ValueError:\n",
    "        print(\"Conversion error, possibly due to unexpected text format.\")\n",
    "        return 0\n",
    "\n",
    "def search_results_more_than_1000(wd):\n",
    "    number = get_search_results_number(wd)\n",
    "    return number > 1000\n",
    "\n",
    "\n",
    "def dataframe_output(wd, search_term, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False, industry=True):\n",
    "    \"\"\"\n",
    "    Conducts a search based on specified parameters and collects results into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    wd (WebDriver): The Selenium WebDriver instance.\n",
    "    search_term (str): The search query.\n",
    "    location (str): Filter by location.\n",
    "    current_company (str): Filter by current company.\n",
    "    past_company (str): Filter by past company.\n",
    "    num_pages (int): Number of pages to scrape.\n",
    "    unlimited (bool): If True, ignores page limits.\n",
    "    industry (bool): If True, applies industry filters if results exceed 1000.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A DataFrame containing the collected data and a list of profile links.\n",
    "    \"\"\"\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    try:\n",
    "        search_query(wd, search_term)  # Assuming search_query now accepts wd as a parameter\n",
    "        \n",
    "        filters_page = add_filters1(wd, location, current_company, past_company)\n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            for i in industry_options:\n",
    "                industry_filter(wd, i)\n",
    "                total_results = get_search_results_number(wd)\n",
    "                sub_df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['Profile Link'])\n",
    "                wd.get(filters_page)\n",
    "        else:\n",
    "            total_results = get_search_results_number(wd)\n",
    "            df = collect_links(wd, 1, num_pages, total_results=total_results, unlimited=unlimited)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        print(\"Exporting current dataframe...\")\n",
    "\n",
    "    links = list(df['Profile Link'])\n",
    "    return df, links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process(wd, keywords, companies, username, password):\n",
    "    login(wd, username, password)\n",
    "    columns = ['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link', 'Keyword', 'Company']\n",
    "    job_df = pd.DataFrame(columns=columns)\n",
    "    keyword = \"\"\n",
    "    company = \"\"\n",
    "    query = \"\"\n",
    "\n",
    "    try:\n",
    "        for keyword in keywords:\n",
    "            timeTotal = 0\n",
    "            for i in range(0, len(companies) - 2, 2):\n",
    "\n",
    "                if i != 0:    \n",
    "                    wd.close()\n",
    "                    print(\"Sleeping for 2 minutes\")\n",
    "                    random_sleep(100, 120)\n",
    "                    wd = refresh_wd()\n",
    "\n",
    "                login(wd, username, password)\n",
    "                for company in companies[i:i+2]:\n",
    "                    query = keyword\n",
    "                    location = \"United States\"\n",
    "                    current_company = company\n",
    "                    past_company = None\n",
    "                    unlimited = False\n",
    "                    num_pages = 5\n",
    "                    print(\"Scraping your request...\")\n",
    "                    print((query, current_company))\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    df, links = dataframe_output(wd, query, location, current_company, past_company, num_pages, unlimited, industry=True)\n",
    "                    print(len(df))\n",
    "                    end_time = time.time()\n",
    "                    timeTotal += (end_time - start_time)\n",
    "                    print(end_time - start_time)\n",
    "\n",
    "                    if len(df) > 0:\n",
    "                        df['Keyword'] = keyword\n",
    "                        df['Company'] = current_company\n",
    "                        df.to_csv(f\"{query.replace(' ', '')}_CompanyDFs/{company.replace(' ', '')}.csv\")\n",
    "                        job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "\n",
    "                    job_df.to_csv(f\"{query.replace(' ', '')}_profiles.csv\")\n",
    "                    wd.get('https://www.linkedin.com/')\n",
    "\n",
    "            print(f\"Average Time per Company Scrape: {(timeTotal / len(companies)) / 60} minutes\")\n",
    "            print(f\"Total Time per Company Scrape: {timeTotal / 60} minutes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if len(df) > 0:\n",
    "            df['Keyword'] = keyword\n",
    "            df['Company'] = company\n",
    "            job_df = pd.concat([job_df, df], ignore_index=True)\n",
    "        job_df.to_csv(f\"{query.replace(' ', '')}_profiles_er.csv\")\n",
    "        print(f'An error occurred: {str(e)} and execution stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "username, password = None, None\n",
    "\n",
    "def set_user_as_PurdueCS(Purdue=True):\n",
    "    global username, password\n",
    "    if Purdue:\n",
    "        username = \"huoerxiu@gmail.com\"\n",
    "        password = \"PURDUEcs\"\n",
    "    else:\n",
    "        username = \"bhat35@purdue.edu\"\n",
    "        password = \"Aymwos@1977!!\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "wd = webdriver.Chrome(options=options)\n",
    "\n",
    "# wd = webdriver.Chrome(ChromeDriverManager().install())\n",
    "actions = ActionChains(wd)\n",
    "\n",
    "wd.maximize_window()\n",
    "wd.switch_to.window(wd.current_window_handle)\n",
    "wd.implicitly_wait(10)\n",
    "\n",
    "wd.get('https://www.linkedin.com/')\n",
    "\n",
    "wd.implicitly_wait(1)\n",
    "\n",
    "keywords = ['Machine Learning Engineer']\n",
    "companies = ['Netflix', \n",
    "             'Amazon',\n",
    "             'Apple', \n",
    "             'Meta', \n",
    "             'Google', \n",
    "             'Microsoft', \n",
    "             'OpenAI',\n",
    "             'Intel',\n",
    "             'Cicsco',\n",
    "             'Nvidia',\n",
    "             'Salesforce',\n",
    "             'LinkedIn',\n",
    "             'DeepMind',\n",
    "             'IBM',\n",
    "             'Bloomberg',\n",
    "             'Tesla',\n",
    "             'Mayo Clinic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def collect_links1(wd, search_term, current_company, state, limit, total_results, unlimited=False, output_file='continuous_data_scrape.csv'):\n",
    "    \n",
    "    state = load_state()\n",
    "    page_to_start_at = state['page_start']\n",
    "\n",
    "    total_results = min(total_results, 1000)  # Limiting the total results to 1000\n",
    "    names, curr_jobs, summarys, locations, links = [], [], [], [], []\n",
    "    print(\"Unlimited:\", unlimited)\n",
    "\n",
    "    ### IMPLEMENT THE BELOW METHOD\n",
    "    if page_to_start_at != 1:\n",
    "        if not go_to_page(wd, page_to_start_at):\n",
    "            print(f\"Could not navigate to page {page_to_start_at}, trying again...\")\n",
    "            if not go_to_page(wd, page_to_start_at):\n",
    "                print(f\"Failed to navigate to page {page_to_start_at}, aborting...\")\n",
    "                return pd.DataFrame()  # Fail gracefully\n",
    "\n",
    "    curr_page = page_to_start_at\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Check for \"Retry Search\" button at the start of each loop iteration\n",
    "            p_names, p_curr_jobs, p_summarys, p_locations, p_links = [], [], [], [], []\n",
    "            handle_retry_search1(wd)\n",
    "\n",
    "            if curr_page % 10 == 0:\n",
    "                print(f\"Scraping links on Page {curr_page}\")\n",
    "\n",
    "            people_list = wd.find_elements(By.CLASS_NAME, \"reusable-search__result-container\")\n",
    "            for index, person in enumerate(people_list):\n",
    "                try:\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid red'\", person)\n",
    "                    collect_person_info1(person, names, curr_jobs, summarys, locations, links) # Adding to general lists\n",
    "                    collect_person_info1(person, p_names, p_curr_jobs, p_summarys, p_locations, p_links) # Adding to lists of specific page\n",
    "                except NoSuchElementException as e:\n",
    "                    print(\"Required Element Not Found...Moving on\")\n",
    "                    print(e)\n",
    "                    wd.execute_script(\"arguments[0].style.border='2px solid green'\", person)\n",
    "                    continue\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'Name': p_names,\n",
    "                'Current Job': p_curr_jobs,\n",
    "                'Relevant Experience to Keyword': p_summarys,\n",
    "                'Location': p_locations,\n",
    "                'Profile Link': p_links\n",
    "            })\n",
    "\n",
    "            df['Keyword'] = search_term\n",
    "            df['Company'] = current_company\n",
    "\n",
    "            if not os.path.isfile(output_file):\n",
    "                df.to_csv(output_file, index=False)  # Create file and write data\n",
    "            else:\n",
    "                df.to_csv(output_file, mode='a', header=False, index=False)  # Append data to existing file\n",
    "\n",
    "            scroll_down1(wd)\n",
    "            if not process_next_page1(wd, curr_page, limit, unlimited):\n",
    "                break\n",
    "\n",
    "            curr_page += 1\n",
    "            state.update({'page_start': curr_page})\n",
    "            save_state(state)\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"An error occurred during the web scraping process: {str(e)}\")\n",
    "        # Handling to save the state or decide what to do next can be added here\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Current Job': curr_jobs,\n",
    "        'Relevant Experience to Keyword': summarys,\n",
    "        'Location': locations,\n",
    "        'Profile Link': links\n",
    "    })\n",
    "\n",
    "def go_to_page(wd, page_number):\n",
    "    try:\n",
    "        # Wait for the pagination container to be visible\n",
    "        scroll_down1(wd)\n",
    "        pagination = WebDriverWait(wd, 5).until(\n",
    "            EC.visibility_of_element_located((By.CLASS_NAME, 'artdeco-pagination__pages'))\n",
    "        )\n",
    "        while True:\n",
    "            pages = pagination.find_elements(By.TAG_NAME, 'button')\n",
    "            # Attempt to find and click the desired page number\n",
    "            page_found = False\n",
    "            for page in pages:\n",
    "                print(page.text)\n",
    "                if page.text.strip() == str(page_number):\n",
    "                    page.click()\n",
    "                    return True\n",
    "\n",
    "            # If not found, find and click the \"...\" button to load more pages\n",
    "            ellipses = [btn for btn in pages if \"...\" in btn.text]\n",
    "            if not ellipses:\n",
    "                print(f\"No '...' found and page {page_number} is not present.\")\n",
    "                return False\n",
    "\n",
    "            # Click the appropriate \"...\" button\n",
    "            # If looking for a higher page number, click the last \"...\" button, else the first\n",
    "            target_ellipsis = ellipses[-1] if page_number > int(pages[-2].text.strip()) else ellipses[0]\n",
    "            target_ellipsis.click()\n",
    "            WebDriverWait(wd, 3).until(EC.staleness_of(target_ellipsis))  # Ensure the pagination has refreshed\n",
    "\n",
    "            # Reassign pagination element to handle potential StaleElementReferenceException\n",
    "            pagination = WebDriverWait(wd, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'artdeco-pagination__pages'))\n",
    "            )\n",
    "    except TimeoutException as e:\n",
    "        print(f\"Failed to navigate to page {page_number}: {e}\")\n",
    "        return False\n",
    "\n",
    "def handle_retry_search1(wd):\n",
    "    try:\n",
    "        retry_button = WebDriverWait(wd, 1).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//button[contains(text(), 'Retry Search')]\"))\n",
    "        )\n",
    "        print(\"Retry Search button found. Waiting before clicking.\")\n",
    "        random_sleep(42, 64)  # Wait for a random time between 42 and 64 seconds\n",
    "        retry_button.click()\n",
    "        print(\"Retry button clicked.\")\n",
    "    except TimeoutException:\n",
    "        not_found = True\n",
    "\n",
    "def collect_person_info1(person, names, curr_jobs, summarys, locations, links):\n",
    "    all_links = person.find_elements(By.TAG_NAME, 'a')\n",
    "    name_text, curr_job_text, link_text, summary_text, location_text = extract_person_details1(person, all_links)\n",
    "\n",
    "    links.append(link_text)\n",
    "    locations.append(location_text)\n",
    "    summarys.append(summary_text)\n",
    "    names.append(name_text)\n",
    "    curr_jobs.append(curr_job_text)\n",
    "\n",
    "    \n",
    "\n",
    "def extract_person_details1(person, all_links):\n",
    "    name_text = \"LinkedIn Member\"\n",
    "\n",
    "    if \"LinkedIn Member\" not in person.text:\n",
    "        name_element = person.find_element(By.CSS_SELECTOR, \".entity-result__title-text.t-16 a span[aria-hidden='true']\")\n",
    "        name_text = name_element.text\n",
    "\n",
    "    curr_jobs = person.find_elements(By.CLASS_NAME, \"entity-result__primary-subtitle.t-14.t-black.t-normal\")\n",
    "    curr_job_text = '\"' + curr_jobs[0].text + '\"' if curr_jobs else \"Null\"\n",
    "\n",
    "    summary_elements = person.find_elements(By.TAG_NAME ,'p')\n",
    "    summary_text = '\"' + summary_elements[0].text + '\"' if summary_elements else \"Null\"\n",
    "\n",
    "    location_element = person.find_element(By.CSS_SELECTOR, \".entity-result__secondary-subtitle.t-14.t-normal\")\n",
    "    location_text = location_element.text\n",
    "\n",
    "    link_text = \"Null\"\n",
    "    for a in all_links:\n",
    "        href = a.get_attribute('href')\n",
    "        if href.startswith(\"https://www.linkedin.com/in\") and not href.startswith(\"https://www.linkedin.com/in/ACo\") and href not in all_links:\n",
    "            link_text = href\n",
    "            break\n",
    "\n",
    "    return name_text, curr_job_text, link_text, summary_text, location_text\n",
    "\n",
    "def scroll_down1(wd):\n",
    "    scroll_script = \"window.scrollBy(0, 2000);\"\n",
    "    wd.execute_script(scroll_script)\n",
    "\n",
    "def process_next_page1(wd, curr_page, limit, unlimited):\n",
    "    print(\"Unlimited: (process_next_page)\", unlimited)\n",
    "    next_page_button = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__button.artdeco-pagination__button--next\")\n",
    "    if not unlimited and curr_page == limit:\n",
    "        return False\n",
    "\n",
    "    if next_page_button.is_enabled():\n",
    "        next_page_button.click()\n",
    "        random_sleep(1, 2)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_state(state):\n",
    "    with open('state.json', 'w') as f:\n",
    "        json.dump(state, f)\n",
    "\n",
    "def load_state():\n",
    "    try:\n",
    "        with open('state.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def dataframe_output1(wd, search_term, location=None, current_company=None, past_company=None, num_pages=4, unlimited=False):\n",
    "    \n",
    "    state = load_state()\n",
    "    industry = state['industry']\n",
    "\n",
    "    df = pd.DataFrame(columns=['Name', 'Current Job', 'Relevant Experience to Keyword', 'Location', 'Profile Link'])\n",
    "    try:\n",
    "        search_query(wd, search_term)\n",
    "        # add_filters1(wd, location, current_company, past_company)\n",
    "        \n",
    "        filters_page = add_filters1(wd, location, current_company, past_company)\n",
    "        print(filters_page)\n",
    "        if industry and search_results_more_than_1000(wd):\n",
    "            industry_options = [0, 1, 2, 3, 4]\n",
    "            industry_options = industry_options[state['industry_number']:]\n",
    "\n",
    "            for i in industry_options:\n",
    "\n",
    "                industry_filter(wd, i)\n",
    "                total_results = get_search_results_number(wd)\n",
    "                sub_df = collect_links1(wd, search_term, current_company, state, num_pages, total_results, unlimited)\n",
    "                df = pd.concat([df, sub_df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['Profile Link'])\n",
    "                print(filters_page)\n",
    "                wd.get(filters_page)\n",
    "                random_sleep(1,2)\n",
    "\n",
    "                state.update({'industry_number': ((i+1)%5)})\n",
    "                save_state(state)\n",
    "        else:\n",
    "            df = collect_links1(wd, search_term, current_company, state, num_pages, 1000, unlimited)\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong: {e}\")\n",
    "        # Here you might want to save the last state or rethrow the exception to handle it in the main scraping function\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def main_scraping_process_rob(wd, keywords, companies, username, password, output_file='scraped_data.csv'):\n",
    "    state = load_state()\n",
    "    if state is None:\n",
    "        state = {'keyword_index': 0, 'company_index': 0, 'page_start': 1, 'industry' : True ,'industry_number': 0, 'problem_locs': {}}\n",
    "        save_state(state)\n",
    "        initial_login = True\n",
    "    else:\n",
    "        initial_login = False\n",
    "\n",
    "    try:\n",
    "        # login(wd, username, password) if initial_login else None\n",
    "        keyword_start_index = state['keyword_index']\n",
    "        company_start_index = state['company_index']\n",
    "        for keyword_index in range(keyword_start_index, len(keywords)):\n",
    "            keyword = keywords[keyword_index]\n",
    "            \n",
    "            for company_index in range(company_start_index, len(companies)):\n",
    "                company = companies[company_index]\n",
    "                state.update({'company_index': company_index, 'page_start': 1})\n",
    "                save_state(state)\n",
    "\n",
    "                try:\n",
    "                    print(f\"Scraping your request for {keyword} at {company}\")\n",
    "                    df = dataframe_output1(\n",
    "                        wd=wd, search_term=keyword, location=\"United States\", current_company=company, past_company=None, num_pages=10, unlimited=True)\n",
    "                    \n",
    "                    df['Keyword'] = keyword\n",
    "                    df['Company'] = company\n",
    "                    if not os.path.isfile(output_file):\n",
    "                        df.to_csv(output_file, index=False)  # Create file and write data\n",
    "                    else:\n",
    "                        df.to_csv(output_file, mode='a', header=False, index=False)  # Append data to existing file\n",
    "\n",
    "                    # Reset page start and last person index after successful scrape\n",
    "                except Exception as e:\n",
    "                    print(f\"Encountered an issue with {company}: {e}, saving state and restarting...\")\n",
    "                    state = load_state()\n",
    "                    print(state)\n",
    "                    \n",
    "                    problem_locs = state['problem_locs']\n",
    "                    current_list = problem_locs.get(keyword_index, [])\n",
    "                    current_list.append(company_index)\n",
    "                    problem_locs[keyword_index] = current_list\n",
    "\n",
    "                    state.update({'problem_locs':problem_locs})\n",
    "                    \n",
    "                    # state.update({'keyword_index': keyword_index, 'company_index': company_index,\n",
    "                    #               'page_start': last_page, 'last_person_index': last_person})\n",
    "                    # save_state(state)\n",
    "                    # wd.quit()\n",
    "                    # wd = refresh_wd()\n",
    "                    # login(wd, username, password)\n",
    "                    # continue\n",
    "                wd.get('https://www.linkedin.com/')\n",
    "\n",
    "            state.update({'company_index': 0})\n",
    "            save_state(state)  # Reset company index after finishing all companies for a keyword\n",
    "\n",
    "        state.update({'keyword_index': 0})  # Reset keyword index after finishing all keywords\n",
    "        save_state(state)\n",
    "    except Exception as e:\n",
    "        print(f'Final error occurred: {str(e)}')\n",
    "\n",
    "    # return pd.read_csv(\"last_saved_dataframe.csv\")  # Assuming your data is saved continuously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bhat35@purdue.edu Aymwos@1977!!\n"
     ]
    }
   ],
   "source": [
    "set_user_as_PurdueCS(Purdue=False)\n",
    "print(username,password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /html/body/div[5]/div[3]/div[2]/div/div[1]/main/div/div/div[5]/div/div/ul\n",
    "# /html/body/div[5]/div[3]/div[2]/div/div[1]/main/div/div/div[5]/div/div/ul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping your request for Machine Learning Engineer at Netflix\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%22165158%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=machine%20learning%20engineer&origin=FACETED_SEARCH&sid=oW%3B\n",
      "Unlimited: True\n",
      "Required Element Not Found...Moving on\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".entity-result__title-text.t-16 a span[aria-hidden='true']\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dc2510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000102dbae58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001029ec19c chromedriver + 278940\n",
      "3   chromedriver                        0x0000000102a2e2c4 chromedriver + 549572\n",
      "4   chromedriver                        0x0000000102a2485c chromedriver + 510044\n",
      "5   chromedriver                        0x0000000102a66c5c chromedriver + 781404\n",
      "6   chromedriver                        0x0000000102a23004 chromedriver + 503812\n",
      "7   chromedriver                        0x0000000102a239ec chromedriver + 506348\n",
      "8   chromedriver                        0x0000000102d8a558 chromedriver + 4072792\n",
      "9   chromedriver                        0x0000000102d8f004 chromedriver + 4091908\n",
      "10  chromedriver                        0x0000000102d7179c chromedriver + 3970972\n",
      "11  chromedriver                        0x0000000102d8f8ec chromedriver + 4094188\n",
      "12  chromedriver                        0x0000000102d6471c chromedriver + 3917596\n",
      "13  chromedriver                        0x0000000102dacb50 chromedriver + 4213584\n",
      "14  chromedriver                        0x0000000102dacccc chromedriver + 4213964\n",
      "15  chromedriver                        0x0000000102dbaa50 chromedriver + 4270672\n",
      "16  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 10\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 20\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 30\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping your request for Machine Learning Engineer at Amazon\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%221586%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=machine%20learning%20engineer&origin=FACETED_SEARCH&sid=I)p\n",
      "Added industry option 0\n",
      "Unlimited: True\n",
      "Required Element Not Found...Moving on\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".entity-result__title-text.t-16 a span[aria-hidden='true']\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dc2510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000102dbae58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001029ec19c chromedriver + 278940\n",
      "3   chromedriver                        0x0000000102a2e2c4 chromedriver + 549572\n",
      "4   chromedriver                        0x0000000102a2485c chromedriver + 510044\n",
      "5   chromedriver                        0x0000000102a66c5c chromedriver + 781404\n",
      "6   chromedriver                        0x0000000102a23004 chromedriver + 503812\n",
      "7   chromedriver                        0x0000000102a239ec chromedriver + 506348\n",
      "8   chromedriver                        0x0000000102d8a558 chromedriver + 4072792\n",
      "9   chromedriver                        0x0000000102d8f004 chromedriver + 4091908\n",
      "10  chromedriver                        0x0000000102d7179c chromedriver + 3970972\n",
      "11  chromedriver                        0x0000000102d8f8ec chromedriver + 4094188\n",
      "12  chromedriver                        0x0000000102d6471c chromedriver + 3917596\n",
      "13  chromedriver                        0x0000000102dacb50 chromedriver + 4213584\n",
      "14  chromedriver                        0x0000000102dacccc chromedriver + 4213964\n",
      "15  chromedriver                        0x0000000102dbaa50 chromedriver + 4270672\n",
      "16  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 10\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 20\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 30\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 40\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 50\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 60\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 70\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 80\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 90\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 100\n",
      "Unlimited: (process_next_page) True\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%221586%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=machine%20learning%20engineer&origin=FACETED_SEARCH&sid=I)p\n",
      "Added industry option 1\n",
      "Unlimited: True\n",
      "Required Element Not Found...Moving on\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".entity-result__title-text.t-16 a span[aria-hidden='true']\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dc2510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000102dbae58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001029ec19c chromedriver + 278940\n",
      "3   chromedriver                        0x0000000102a2e2c4 chromedriver + 549572\n",
      "4   chromedriver                        0x0000000102a2485c chromedriver + 510044\n",
      "5   chromedriver                        0x0000000102a66c5c chromedriver + 781404\n",
      "6   chromedriver                        0x0000000102a23004 chromedriver + 503812\n",
      "7   chromedriver                        0x0000000102a239ec chromedriver + 506348\n",
      "8   chromedriver                        0x0000000102d8a558 chromedriver + 4072792\n",
      "9   chromedriver                        0x0000000102d8f004 chromedriver + 4091908\n",
      "10  chromedriver                        0x0000000102d7179c chromedriver + 3970972\n",
      "11  chromedriver                        0x0000000102d8f8ec chromedriver + 4094188\n",
      "12  chromedriver                        0x0000000102d6471c chromedriver + 3917596\n",
      "13  chromedriver                        0x0000000102dacb50 chromedriver + 4213584\n",
      "14  chromedriver                        0x0000000102dacccc chromedriver + 4213964\n",
      "15  chromedriver                        0x0000000102dbaa50 chromedriver + 4270672\n",
      "16  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 10\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 20\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 30\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 40\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 50\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 60\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 70\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 80\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 90\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 100\n",
      "Unlimited: (process_next_page) True\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%221586%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=machine%20learning%20engineer&origin=FACETED_SEARCH&sid=I)p\n",
      "Added industry option 2\n",
      "Unlimited: True\n",
      "Required Element Not Found...Moving on\n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".entity-result__title-text.t-16 a span[aria-hidden='true']\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dc2510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000102dbae58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001029ec19c chromedriver + 278940\n",
      "3   chromedriver                        0x0000000102a2e2c4 chromedriver + 549572\n",
      "4   chromedriver                        0x0000000102a2485c chromedriver + 510044\n",
      "5   chromedriver                        0x0000000102a66c5c chromedriver + 781404\n",
      "6   chromedriver                        0x0000000102a23004 chromedriver + 503812\n",
      "7   chromedriver                        0x0000000102a239ec chromedriver + 506348\n",
      "8   chromedriver                        0x0000000102d8a558 chromedriver + 4072792\n",
      "9   chromedriver                        0x0000000102d8f004 chromedriver + 4091908\n",
      "10  chromedriver                        0x0000000102d7179c chromedriver + 3970972\n",
      "11  chromedriver                        0x0000000102d8f8ec chromedriver + 4094188\n",
      "12  chromedriver                        0x0000000102d6471c chromedriver + 3917596\n",
      "13  chromedriver                        0x0000000102dacb50 chromedriver + 4213584\n",
      "14  chromedriver                        0x0000000102dacccc chromedriver + 4213964\n",
      "15  chromedriver                        0x0000000102dbaa50 chromedriver + 4270672\n",
      "16  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "17  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 10\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 20\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Scraping links on Page 30\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "Unlimited: (process_next_page) True\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%221586%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=machine%20learning%20engineer&origin=FACETED_SEARCH&sid=I)p\n",
      "Added industry option 3\n",
      "Unlimited: True\n",
      "Unlimited: (process_next_page) True\n",
      "An error occurred during the web scraping process: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".artdeco-pagination__button.artdeco-pagination__button--next\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dc2510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000102dbae58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001029ec19c chromedriver + 278940\n",
      "3   chromedriver                        0x0000000102a2e2c4 chromedriver + 549572\n",
      "4   chromedriver                        0x0000000102a66c5c chromedriver + 781404\n",
      "5   chromedriver                        0x0000000102a23004 chromedriver + 503812\n",
      "6   chromedriver                        0x0000000102a239ec chromedriver + 506348\n",
      "7   chromedriver                        0x0000000102d8a558 chromedriver + 4072792\n",
      "8   chromedriver                        0x0000000102d8f004 chromedriver + 4091908\n",
      "9   chromedriver                        0x0000000102d7179c chromedriver + 3970972\n",
      "10  chromedriver                        0x0000000102d8f8ec chromedriver + 4094188\n",
      "11  chromedriver                        0x0000000102d6471c chromedriver + 3917596\n",
      "12  chromedriver                        0x0000000102dacb50 chromedriver + 4213584\n",
      "13  chromedriver                        0x0000000102dacccc chromedriver + 4213964\n",
      "14  chromedriver                        0x0000000102dbaa50 chromedriver + 4270672\n",
      "15  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%221586%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=machine%20learning%20engineer&origin=FACETED_SEARCH&sid=I)p\n",
      "Added industry option 4\n",
      "Unlimited: True\n",
      "Unlimited: (process_next_page) True\n",
      "An error occurred during the web scraping process: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".artdeco-pagination__button.artdeco-pagination__button--next\"}\n",
      "  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dc2510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000102dbae58 chromedriver + 4271704\n",
      "2   chromedriver                        0x00000001029ec19c chromedriver + 278940\n",
      "3   chromedriver                        0x0000000102a2e2c4 chromedriver + 549572\n",
      "4   chromedriver                        0x0000000102a66c5c chromedriver + 781404\n",
      "5   chromedriver                        0x0000000102a23004 chromedriver + 503812\n",
      "6   chromedriver                        0x0000000102a239ec chromedriver + 506348\n",
      "7   chromedriver                        0x0000000102d8a558 chromedriver + 4072792\n",
      "8   chromedriver                        0x0000000102d8f004 chromedriver + 4091908\n",
      "9   chromedriver                        0x0000000102d7179c chromedriver + 3970972\n",
      "10  chromedriver                        0x0000000102d8f8ec chromedriver + 4094188\n",
      "11  chromedriver                        0x0000000102d6471c chromedriver + 3917596\n",
      "12  chromedriver                        0x0000000102dacb50 chromedriver + 4213584\n",
      "13  chromedriver                        0x0000000102dacccc chromedriver + 4213964\n",
      "14  chromedriver                        0x0000000102dbaa50 chromedriver + 4270672\n",
      "15  libsystem_pthread.dylib             0x0000000181813034 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x000000018180de3c thread_start + 8\n",
      "\n",
      "https://www.linkedin.com/search/results/people/?currentCompany=%5B%221586%22%5D&geoUrn=%5B%22103644278%22%5D&keywords=machine%20learning%20engineer&origin=FACETED_SEARCH&sid=I)p\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# main_scraping_process(wd, keywords, companies, username, password)846448\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m main_scraping_process_rob(wd, keywords, companies, username, password)\n",
      "\u001b[1;32m/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb Cell 10\u001b[0m in \u001b[0;36mmain_scraping_process_rob\u001b[0;34m(wd, keywords, companies, username, password, output_file)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mprint\u001b[39m(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         \u001b[39m# state.update({'keyword_index': keyword_index, 'company_index': company_index,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39m#               'page_start': last_page, 'last_person_index': last_person})\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         \u001b[39m# save_state(state)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         \u001b[39m# login(wd, username, password)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         \u001b[39m# continue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     wd\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mhttps://www.linkedin.com/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m state\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mcompany_index\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishnatejbhat/Documents/Research/LinkedInScraping/LinkedInScraping/LinkedInScrape.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m save_state(state)  \u001b[39m# Reset company index after finishing all companies for a keyword\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     \u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    343\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m--> 345\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m trimmed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    301\u001b[0m LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, command_info[\u001b[39m0\u001b[39m], url, \u001b[39mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 302\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    323\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main_scraping_process(wd, keywords, companies, username, password)846448\n",
    "main_scraping_process_rob(wd, keywords, companies, username, password)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be8df3a746dc770fc8ec50a8a16d0a923a6575bbbcf271bc91914f68c2488458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
